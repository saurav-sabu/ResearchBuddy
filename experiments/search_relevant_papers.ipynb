{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8eecd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import ArxivLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f616d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_arxiv_papers(query, max_results=2):\n",
    "    # Initialize ArxivLoader with search query and maximum number of results\n",
    "    loader = ArxivLoader(query=query, max_results=max_results)\n",
    "    \n",
    "    # Load the documents from ArXiv\n",
    "    docs = loader.load()\n",
    "\n",
    "    # Print the number of documents found\n",
    "    print(f\"Loaded {len(docs)} documents\\n\")\n",
    "\n",
    "    # Iterate through each document and print its metadata\n",
    "    for i, doc in enumerate(docs, 1):\n",
    "        # Print full metadata for debugging/reference\n",
    "        print(f\"üìò Paper {i}:\\n{doc.metadata}\\n\")\n",
    "        \n",
    "        # Print specific metadata fields of interest\n",
    "        print(f\"üìò Title:\\n{doc.metadata[\"Title\"]}\\n\")\n",
    "        print(f\"üìò Published::\\n{doc.metadata[\"Published\"]}\\n\")\n",
    "        print(f\"üìò Authors::\\n{doc.metadata[\"Authors\"]}\\n\")\n",
    "\n",
    "        print(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4594ddb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 documents\n",
      "\n",
      "üìò Paper 1:\n",
      "{'Published': '2024-07-22', 'Title': \"Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\", 'Authors': 'Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini', 'Summary': 'The inference demand for LLMs has skyrocketed in recent months, and serving\\nmodels with low latencies remains challenging due to the quadratic input length\\ncomplexity of the attention layers. In this work, we investigate the effect of\\ndropping MLP and attention layers at inference time on the performance of\\nLlama-v2 models. We find that dropping dreeper attention layers only marginally\\ndecreases performance but leads to the best speedups alongside dropping entire\\nlayers. For example, removing 33\\\\% of attention layers in a 13B Llama2 model\\nresults in a 1.8\\\\% drop in average performance over the OpenLLM benchmark. We\\nalso observe that skipping layers except the latter layers reduces performances\\nfor more layers skipped, except for skipping the attention layers.'}\n",
      "\n",
      "üìò Title:\n",
      "Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\n",
      "\n",
      "üìò Published::\n",
      "2024-07-22\n",
      "\n",
      "üìò Authors::\n",
      "Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini\n",
      "\n",
      "Attention Is All You Need But You Don‚Äôt Need All Of It\n",
      "For Inference of Large Language Models\n",
      "Georgy Tyukin * 1 Gbetondji J-S Dovonon 1 Jean Kaddour 1 Pasquale Minervini 2\n",
      "Abstract\n",
      "The inference demand for LLMs has skyrocketed\n",
      "in recent months, and serving models with low\n",
      "latencies remains challenging due to the quadratic\n",
      "input length complexity of the attention layers.\n",
      "In this work, we investigate the effect of drop-\n",
      "ping MLP and attention layers at inference time\n",
      "on the performance of Llama-v2 models. We\n",
      "find that dropping dreeper attention layers only\n",
      "marginally decreases performance but leads to the\n",
      "best speedups alongside dropping entire layers.\n",
      "For example, removing 33% of attention layers\n",
      "in a 13B Llama2 model results in a 1.8% drop in\n",
      "average performance over the OpenLLM bench-\n",
      "mark. We also observe that skipping layers except\n",
      "the latter layers reduces performances for more\n",
      "layers skipped, except for skipping the attention\n",
      "layers.\n",
      "1. Introduction\n",
      "The ubiquitous deployment of Large Language Models\n",
      "(LLMs) results in ever-growing amounts of compute spent\n",
      "on inference (Patterson et al., 2021; Chen et al., 2023; Kad-\n",
      "dour et al., 2023a; Xia et al., 2024; Reid et al., 2024). Fur-\n",
      "ther, serving models with low latencies remains challenging\n",
      "because contemporary Transformer architectures employ\n",
      "the self-attention mechanism with quadratic input complex-\n",
      "ity (Touvron et al., 2023b; Jiang et al., 2023; Bi et al., 2024).\n",
      "In this work, we delve deeper into the concept of layer\n",
      "skipping (Fan et al., 2019; Wang et al., 2022a) to reduce\n",
      "the computation on superfluous LLM components. Our\n",
      "findings demonstrate that pruning deeper attention layers\n",
      "does not significantly affect performance. When applied\n",
      "to Llama-v2 (Touvron et al., 2023b), we maintain good\n",
      "performance on the OpenLLM (ARC (Clark et al., 2018),\n",
      "*Equal contribution\n",
      "1University College London,\n",
      "UK\n",
      "2University of Edinburgh, UK. Correspondence to: Georgy Tyukin\n",
      "<tyukinegor@gmail.com>.\n",
      "Work presented at TF2M workshop at ICML 2024, Vienna, Austria.\n",
      "PMLR 235, 2024. Copyright 2024 by the author(s).\n",
      "HellaSwag (Zellers et al., 2019), MMLU (Hendrycks et al.,\n",
      "2021), TruthfulQA (Lin et al., 2022)) benchmarks (Beech-\n",
      "ing et al., 2023), recording only minimal performance devi-\n",
      "ations compared to the full model.\n",
      "2. Method\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "Layer\n",
      "0.65\n",
      "0.70\n",
      "0.75\n",
      "0.80\n",
      "0.85\n",
      "0.90\n",
      "0.95\n",
      "1.00\n",
      "Cosine Similarity\n",
      "Cosine Similarity with previous layer for LLaMA-v2 7b and LLaMA-v2 13\n",
      "LLaMA-v2 7b\n",
      "LLaMA-v2 13b\n",
      "Figure 1. Cosine similarity of Llama-v2 layers with the previous\n",
      "layer: We observe that the deeper the layer, the more its features\n",
      "are similar to the previous layer except for the very last layer.\n",
      "2.1. Layer skipping\n",
      "Consider a Transformer model M with L layers, each\n",
      "consisting of an attention sub-layer followed by a multi-\n",
      "layer perceptron (MLP) sub-layer. We denote each layer as\n",
      "Mi = (Attentioni, MLPi) for i ‚àà{1, 2, . . . , L}.\n",
      "To compare the performance of Transformer models when\n",
      "skipping specific sub-layers, we create two variants of the\n",
      "model:\n",
      "1. Skipping MLP Layers: We construct a model Mskip MLP\n",
      "1\n",
      "arXiv:2407.15516v1  [cs.LG]  22 Jul 2024\n",
      "Attention Is All You Need But You Don‚Äôt Need All Of It\n",
      "by skipping the MLP sub-layer from the last k layers. The\n",
      "resulting model is Mskip MLP = {(Attentioni, MLPi) | i ‚àà\n",
      "{1, 2, . . . , L ‚àík}} ‚à™{(Attentioni, ‚àÖ) | i ‚àà{L ‚àík +\n",
      "1, . . . , L}}.\n",
      "2. Skipping Attention Layers: We construct a model\n",
      "Mskip Attention by skipping the attention sub-layer from the\n",
      "last k layers.\n",
      "The resulting model is Mskip Attention =\n",
      "{(Attentioni, MLPi)\n",
      "|\n",
      "i\n",
      "‚àà\n",
      "{1, 2, . . . , L ‚àík}} ‚à™\n",
      "{(‚àÖ, MLPi) | i ‚àà{L ‚àík + 1, . . . , L}}.\n",
      "3. Skipping Transformer Blocks: We construct a model\n",
      "Mskip Attention by skipping the entire last k layers. The re-\n",
      "sulting model is Mskip Block = {(Attentioni, MLPi) | i ‚àà\n",
      "{1, 2, . . . , L ‚àík}} ‚à™{(‚àÖ) | i ‚àà{L ‚àík + 1, . . . , L}}.\n",
      "We then evaluate the performance of these modified models\n",
      "on the OpenLLM benchmark (Beeching et al., 2023), com-\n",
      "paring metrics such as accuracy, computational efficiency,\n",
      "and memory usage. This comparison helps in understand-\n",
      "ing the individual contributions of the attention and MLP\n",
      "sub-layers to the overall performance of the Transformer\n",
      "model.\n",
      "(a) Skip attention lay-\n",
      "ers.\n",
      "(b) Skip attention lay-\n",
      "ers,\n",
      "keep last full\n",
      "block.\n",
      "(c) Skip ffwd layers.\n",
      "(d) Skip ffwd layers,\n",
      "keep last full block.\n",
      "(e) Skip full blocks.\n",
      "(f) Skip full blocks,\n",
      "keep last full block.\n",
      "Figure 2. Skip mechanisms for skipping single layers and entire\n",
      "Transformer blocks (ffwd and attention layers) during inference.\n",
      "2.2. Motivation: Are Deeper Layers More Redundant?\n",
      "In Transformer models, the last layers have been shown to\n",
      "contribute less information than earlier layers, making it\n",
      "possible to drop those layers at a minimal performance cost\n",
      "(Fan et al., 2019; Zhang & He, 2020; Wang et al., 2022a;\n",
      "Schuster et al., 2022; Kaddour et al., 2023b; Belrose et al.,\n",
      "2023).\n",
      "To verify this, we experiment with removing either the at-\n",
      "tention sublayers or the MLP sublayers. Figure 1 shows the\n",
      "cosine similarities between a layer‚Äôs features and the previ-\n",
      "ous layer showing that deeper layers have a lower impact\n",
      "on the features than earlier layers. One notable exception\n",
      "to this trend is that the last layer for both Llama-v2 7B and\n",
      "13B has the lowest cosine similarity with the previous layer.\n",
      "Previous analysis of the attention mechanism has shown\n",
      "that they can converge to the same value due to attention\n",
      "collapse (Zhai et al., 2023) and token features that also con-\n",
      "verge to the same value due to over-smoothing (Wang et al.,\n",
      "2022b; Dovonon et al., 2024) or rank collapse (Dong et al.,\n",
      "2023), with solutions to these issues typically improving\n",
      "performance (Ali et al., 2023; Choi et al., 2024).\n",
      "3. Results\n",
      "Experimental Setup\n",
      "For all experiments, we use either\n",
      "Llama-v2-7B or Llama-v2-13B (Touvron et al., 2023a;b),\n",
      "two LLMs trained on trillions of publically available tokens.\n",
      "We experiment with keeping 66%, 75%, 90% and 100% of\n",
      "the network and report the corresponding results in Table 1.\n",
      "We also experiment with removing attention sublayers in\n",
      "Table 2, MLP sublayers in Table 3, and a varying number of\n",
      "layers similar to Table 1 but keeping the last layer in Table 4.\n",
      "3.1. Chopping Layers\n",
      "Table 1. Llama-v2 skipping full layer\n",
      "Model\n",
      "Performances\n",
      "ARC\n",
      "HellaSwag\n",
      "TruthfulQA\n",
      "MMLU\n",
      "Average\n",
      "7B-66%\n",
      "35.2\n",
      "46.8\n",
      "46.2\n",
      "40.3\n",
      "42.1\n",
      "7B-75%\n",
      "38.3\n",
      "53.0\n",
      "45.1\n",
      "45.9\n",
      "45.6\n",
      "7B-90%\n",
      "47.7\n",
      "69.3\n",
      "39.6\n",
      "46.4\n",
      "50.8\n",
      "7B-100%\n",
      "53.1\n",
      "78.6\n",
      "38.8\n",
      "46.6\n",
      "54.3\n",
      "13B-66%\n",
      "37.8\n",
      "46.8\n",
      "45.3\n",
      "51.8\n",
      "45.4\n",
      "13B-75%\n",
      "40.9\n",
      "53.6\n",
      "42.5\n",
      "53.2\n",
      "47.6\n",
      "13B-90%\n",
      "51.3\n",
      "71.3\n",
      "37.1\n",
      "54.8\n",
      "53.6\n",
      "13B-100%\n",
      "59.6\n",
      "82.1\n",
      "36.9\n",
      "55.4\n",
      "58.5\n",
      "On all datasets except TruthfulQA, performance drops\n",
      "which is expected. It had already been observed that larger\n",
      "language models are less truthful (Lin et al., 2022), but we\n",
      "now also observe that reducing the size of already trained\n",
      "models can also make them more truthful. The observa-\n",
      "tion still holds when the last layer is preserved. Skipping\n",
      "2\n",
      "Attention Is All You Need But You Don‚Äôt Need All Of It\n",
      "Table 2. Llama-v2 skipping attention sublayers\n",
      "Model\n",
      "Performances\n",
      "ARC\n",
      "HellaSwag\n",
      "TruthfulQA\n",
      "MMLU\n",
      "Average\n",
      "7B-66%\n",
      "51.2\n",
      "77.0\n",
      "42.2\n",
      "39.4\n",
      "52.5\n",
      "7B-75%\n",
      "52.5\n",
      "78.3\n",
      "42.3\n",
      "41.4\n",
      "53.6\n",
      "7B-90%\n",
      "52.8\n",
      "78.9\n",
      "40.0\n",
      "44.0\n",
      "53.9\n",
      "7B-100%\n",
      "53.1\n",
      "78.6\n",
      "38.8\n",
      "46.6\n",
      "54.3\n",
      "13B-66%\n",
      "55.6\n",
      "80.1\n",
      "40.1\n",
      "51.3\n",
      "56.8\n",
      "13B-75%\n",
      "55.9\n",
      "79.7\n",
      "39.9\n",
      "52.1\n",
      "56.9\n",
      "13B-90%\n",
      "57.0\n",
      "81.3\n",
      "38.2\n",
      "54.8\n",
      "57.8\n",
      "13B-100%\n",
      "59.6\n",
      "82.1\n",
      "36.9\n",
      "55.4\n",
      "58.5\n",
      "Table 3. Llama-v2 skipping ffwd sublayers\n",
      "Model\n",
      "Performances\n",
      "ARC\n",
      "HellaSwag\n",
      "TruthfulQA\n",
      "MMLU\n",
      "Average\n",
      "7B-66%\n",
      "35.1\n",
      "52.5\n",
      "42.2\n",
      "43.9\n",
      "43.4\n",
      "7B-75%\n",
      "40.4\n",
      "60.3\n",
      "39.2\n",
      "46.3\n",
      "46.6\n",
      "7B-90%\n",
      "48.5\n",
      "71.4\n",
      "38.0\n",
      "46.1\n",
      "51.0\n",
      "7B-100%\n",
      "53.1\n",
      "78.6\n",
      "38.8\n",
      "46.6\n",
      "54.3\n",
      "13B-66%\n",
      "41.6\n",
      "56.9\n",
      "40.7\n",
      "53.4\n",
      "48.2\n",
      "13B-75%\n",
      "47.3\n",
      "65.2\n",
      "40.0\n",
      "53.2\n",
      "51.4\n",
      "13B-90%\n",
      "54.2\n",
      "75.8\n",
      "38.3\n",
      "54.7\n",
      "55.8\n",
      "13B-100%\n",
      "59.6\n",
      "82.1\n",
      "36.9\n",
      "55.4\n",
      "58.5\n",
      "attention layers only leads to better results with only a 1.8%\n",
      "decrease in performance when keeping 66% of the network\n",
      "compared to a 13.1% decrease in performance when drop-\n",
      "ping dropping the MLP layers only. This seems to indicate\n",
      "that MLP layers are more important than attention layers, at\n",
      "least in deeper parts of the network.\n",
      "3.2. Last Layer Inclusion\n",
      "Table 4. Llama-v2 skip full layers with last layer\n",
      "Model\n",
      "Performances\n",
      "ARC\n",
      "HellaSwag\n",
      "TruthfulQA\n",
      "MMLU\n",
      "Average\n",
      "7B-66%\n",
      "32.0\n",
      "45.8\n",
      "46.9\n",
      "40.7\n",
      "41.3\n",
      "7B-75%\n",
      "34.5\n",
      "49.4\n",
      "45.9\n",
      "38.3\n",
      "42.0\n",
      "7B-90%\n",
      "46.5\n",
      "73.1\n",
      "41.8\n",
      "41.4\n",
      "50.7\n",
      "7B-100%\n",
      "53.1\n",
      "78.6\n",
      "38.8\n",
      "46.6\n",
      "54.3\n",
      "13B-66%\n",
      "35.1\n",
      "50.0\n",
      "46.9\n",
      "19.1\n",
      "37.8\n",
      "13B-75%\n",
      "38.7\n",
      "56.6\n",
      "43.7\n",
      "25.2\n",
      "41.1\n",
      "13B-90%\n",
      "51.2\n",
      "78.1\n",
      "38.0\n",
      "27.1\n",
      "47.9\n",
      "13B-100%\n",
      "59.6\n",
      "82.1\n",
      "36.9\n",
      "55.4\n",
      "58.5\n",
      "Surprisingly, we notice that skipping layers except the lat-\n",
      "ter layers reduces performances for more layers skipped,\n",
      "except for skipping the attention layers. This is even more\n",
      "exaggerated compared to just dropping layers, including the\n",
      "last one. The reason for this could be attributed to the (lack\n",
      "of) robustness of feedforward sublayers, as the last layer\n",
      "now has to process perturbed information from earlier lay-\n",
      "ers. For future work, it would be interesting to see if these\n",
      "performance drops can be compensated by a small amount\n",
      "Table 5. Llama-v2 skip attention sublayers with last layer\n",
      "Model\n",
      "Performances\n",
      "ARC\n",
      "HellaSwag\n",
      "TruthfulQA\n",
      "MMLU\n",
      "Average\n",
      "7B-66%\n",
      "49.3\n",
      "77.1\n",
      "40.5\n",
      "42.5\n",
      "52.4\n",
      "7B-75%\n",
      "51.8\n",
      "78.3\n",
      "41.1\n",
      "44.1\n",
      "53.8\n",
      "7B-90%\n",
      "51.9\n",
      "78.7\n",
      "39.4\n",
      "45.7\n",
      "53.9\n",
      "7B-100%\n",
      "53.1\n",
      "78.6\n",
      "38.8\n",
      "46.6\n",
      "54.3\n",
      "13B-66%\n",
      "56.8\n",
      "82.1\n",
      "38.0\n",
      "50.3\n",
      "56.8\n",
      "13B-75%\n",
      "57.5\n",
      "82.1\n",
      "37.0\n",
      "51.4\n",
      "57.0\n",
      "13B-90%\n",
      "58.9\n",
      "82.4\n",
      "36.6\n",
      "54.5\n",
      "58.1\n",
      "13B-100%\n",
      "59.6\n",
      "82.1\n",
      "36.9\n",
      "55.4\n",
      "58.5\n",
      "Table 6. Llama-v2 skip ffwd sublayers with last layer\n",
      "Model\n",
      "Performances\n",
      "ARC\n",
      "HellaSwag\n",
      "TruthfulQA\n",
      "MMLU\n",
      "Average\n",
      "7B-66%\n",
      "32.0\n",
      "45.8\n",
      "46.9\n",
      "39.4\n",
      "41.0\n",
      "7B-75%\n",
      "34.5\n",
      "49.4\n",
      "45.9\n",
      "40.2\n",
      "42.5\n",
      "7B-90%\n",
      "46.5\n",
      "73.1\n",
      "41.8\n",
      "40.2\n",
      "50.4\n",
      "7B-100%\n",
      "53.1\n",
      "78.6\n",
      "38.8\n",
      "46.6\n",
      "54.3\n",
      "13B-66%\n",
      "35.1\n",
      "50.0\n",
      "46.9\n",
      "20.4\n",
      "38.1\n",
      "13B-75%\n",
      "38.7\n",
      "56.6\n",
      "43.7\n",
      "33.6\n",
      "43.2\n",
      "13B-90%\n",
      "51.2\n",
      "78.1\n",
      "38.0\n",
      "34.4\n",
      "50.4\n",
      "13B-100%\n",
      "59.6\n",
      "82.1\n",
      "36.9\n",
      "55.4\n",
      "58.5\n",
      "of continued training; since model growing techniques for\n",
      "training seem to not suffer from instabilities (Kaddour et al.,\n",
      "2023b).\n",
      "3.3. Compute-matched Comparison\n",
      "To measure the efficiency of the networks we conducted\n",
      "a separate experiment, where we record the time it takes\n",
      "for the model to output a sequence of length 1, averaging\n",
      "over 1000 sequences. We conducted this experiment for\n",
      "both 50 and 100 length input sequences. We notice that full\n",
      "layer droppings do improve time costs the best, followed by\n",
      "attention sublayers, and then feedforward sublayers which\n",
      "do not impact the speed of processing a lot.\n",
      "We report the time√ó102 (for clarity) it takes to predict 1\n",
      "token for 1000 sequences as well as the percentage improve-\n",
      "ment. We show the results of this experiment for Llama 2\n",
      "7B with 0%, 10%, 25%, 33% of layers skipped and we label\n",
      "these as 7B-100%, 7B-90%, 7B-75%, 7B-66% respectively.\n",
      "Table 7. Llama-v2 time results, 50 length sequence, no last layer\n",
      "Model\n",
      "Full\n",
      "Attention\n",
      "ffwd\n",
      "Time(s) √ó102\n",
      "(%)\n",
      "Time(s) √ó102\n",
      "(%)\n",
      "Time(s) √ó102\n",
      "(%)\n",
      "7B-66%\n",
      "31.35\n",
      "32.96\n",
      "36.72\n",
      "21.47\n",
      "43.51\n",
      "6.95\n",
      "7B-75%\n",
      "35.48\n",
      "24.12\n",
      "39.46\n",
      "15.61\n",
      "42.88\n",
      "8.30\n",
      "7B-90%\n",
      "43.31\n",
      "7.38\n",
      "42.93\n",
      "8.19\n",
      "44.17\n",
      "5.53\n",
      "7B-100%\n",
      "46.76\n",
      "0\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "3\n",
      "Attention Is All You Need But You Don‚Äôt Need All Of It\n",
      "Table 8. Llama-v2 time results, 50 length sequence, last layer in-\n",
      "cluded\n",
      "Model\n",
      "Full\n",
      "Attention\n",
      "ffwd\n",
      "Time(s) √ó102\n",
      "(%)\n",
      "Time(s) √ó102\n",
      "(%)\n",
      "Time(s) √ó102\n",
      "(%)\n",
      "7B-66%\n",
      "31.78\n",
      "32.04\n",
      "36.92\n",
      "21.04\n",
      "41.31\n",
      "11.66\n",
      "7B-75%\n",
      "34.98\n",
      "25.19\n",
      "40.24\n",
      "13.94\n",
      "42.62\n",
      "8.85\n",
      "7B-90%\n",
      "40.92\n",
      "12.49\n",
      "42.43\n",
      "9.26\n",
      "43.51\n",
      "6.95\n",
      "7B-100%\n",
      "46.76\n",
      "0\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "Table 9. Llama-v2 time results, 100 length sequence, no last layer\n",
      "Model\n",
      "Full\n",
      "Attention\n",
      "ffwd\n",
      "Time(s) √ó102\n",
      "(%)\n",
      "Time(s) √ó102\n",
      "(%)\n",
      "Time(s) √ó102\n",
      "(%)\n",
      "7B-66%\n",
      "32.36\n",
      "32.58\n",
      "38.97\n",
      "18.18\n",
      "43.08\n",
      "10.25\n",
      "7B-75%\n",
      "36.58\n",
      "23.79\n",
      "41.27\n",
      "14.02\n",
      "44.13\n",
      "8.06\n",
      "7B-90%\n",
      "43.65\n",
      "9.06\n",
      "44.62\n",
      "7.04\n",
      "46.30\n",
      "3.54\n",
      "7B-100%\n",
      "48.00\n",
      "0\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "Table 10. Llama-v2 time results, 100 length sequence, last layer\n",
      "included\n",
      "Model\n",
      "Full\n",
      "Attention\n",
      "ffwd\n",
      "Time(s) √ó102\n",
      "(%)\n",
      "Time(s) √ó102\n",
      "(%)\n",
      "Time(s) √ó102\n",
      "(%)\n",
      "7B-66%\n",
      "32.05\n",
      "33.23\n",
      "38.52\n",
      "19.75\n",
      "42.66\n",
      "11.13\n",
      "7B-75%\n",
      "36.41\n",
      "24.15\n",
      "41.00\n",
      "14.58\n",
      "43.92\n",
      "8.50\n",
      "7B-90%\n",
      "43.28\n",
      "9.83\n",
      "44.27\n",
      "7.77\n",
      "45.20\n",
      "5.83\n",
      "7B-100%\n",
      "48.00\n",
      "0\n",
      "-\n",
      "-\n",
      "-\n",
      "-\n",
      "4. Related Work\n",
      "Early Exit during inference\n",
      "Early exit methods have also\n",
      "been proposed in other domains (Graves, 2017; Teerapit-\n",
      "tayanon et al., 2017) before getting adapted to autoregressive\n",
      "models (Elbayad et al., 2020; Schuster et al., 2022; Din et al.,\n",
      "2023; Elhoushi et al., 2024; Fan et al., 2024; Chen et al.,\n",
      "2024). The idea works by dynamically allocating compute\n",
      "based on the difficulty of the input sequence. Our method\n",
      "prunes the deepest layers and does not involve any level of\n",
      "adaptability. This is beneficial because it does not require\n",
      "the entire model to be loaded in memory. Dropping layers\n",
      "during inference has been done on BERT-like models in\n",
      "(Wang et al., 2022a; Sajjad et al., 2023). We apply a similar\n",
      "analysis to more recent LLMs and study the impact of skip-\n",
      "ping attention and/or MLP layers in more detail. Concurrent\n",
      "work to ours by Gromov et al. (2024) yields similar results\n",
      "by pruning deeper layers and applying fine-tuning on the\n",
      "pruned model.\n",
      "Layer dropping/growing during training\n",
      "There are var-\n",
      "ious works studying the dropping/growing layers dynami-\n",
      "cally during training (Fan et al., 2019; Gong et al., 2019;\n",
      "Kaddour et al., 2023b; Jiang et al., 2020; Liu et al., 2023). In\n",
      "contrast, this work focuses on dropping layers of an already\n",
      "pre-trained model in a way similar to Men et al. (2024).\n",
      "Other Inference Speedup Methods\n",
      "Other works to speed\n",
      "up inference include compressing KV caches (Nawrot et al.,\n",
      "2024; Wu & Tu, 2024; Bi et al., 2024), speculative decoding\n",
      "(Chen et al., 2023), efficient memory management (Kwon\n",
      "et al., 2023), or subqudratic attention architectures (Fu et al.,\n",
      "2022; Peng et al., 2023; Gu & Dao, 2023), an overview has\n",
      "been provided by Kaddour et al. (2023a).\n",
      "5. Conclusion\n",
      "We investigated the effect of dropping the last layers from\n",
      "the 7B and 13B Llama2 models. We observe that dropping\n",
      "attention sublayers lead to much lower drops in performance\n",
      "than dropping the MLP sublayers, whether the last layer\n",
      "is included or not, while also leading to better inference\n",
      "speedups. For example, removing 33% of attention layers\n",
      "leads to an 18% speedup in a 13B Llama2 model at the cost\n",
      "of a 1.8% drop in average performance. This shows that\n",
      "massive improvements can be made over dropping entire\n",
      "layers from just dropping the attention sublayer.\n",
      "References\n",
      "Ali, A., Galanti, T., and Wolf, L. Centered self-attention\n",
      "layers, 2023.\n",
      "Beeching,\n",
      "E.,\n",
      "Fourrier,\n",
      "C.,\n",
      "Habib,\n",
      "N.,\n",
      "Han,\n",
      "S.,\n",
      "Lambert,\n",
      "N.,\n",
      "Rajani,\n",
      "N.,\n",
      "Sanseviero,\n",
      "O.,\n",
      "Tun-\n",
      "stall,\n",
      "L.,\n",
      "and\n",
      "Wolf,\n",
      "T.\n",
      "Open\n",
      "llm\n",
      "leader-\n",
      "board.\n",
      "https://huggingface.co/spaces/\n",
      "HuggingFaceH4/open_llm_leaderboard,\n",
      "2023.\n",
      "Belrose, N., Furman, Z., Smith, L., Halawi, D., Ostrovsky, I.,\n",
      "McKinney, L., Biderman, S., and Steinhardt, J. Eliciting\n",
      "latent predictions from transformers with the tuned lens.\n",
      "arXiv preprint arXiv:2303.08112, 2023.\n",
      "Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C.,\n",
      "Ding, H., Dong, K., Du, Q., Fu, Z., et al. Deepseek llm:\n",
      "Scaling open-source language models with longtermism.\n",
      "arXiv preprint arXiv:2401.02954, 2024.\n",
      "Chen, C., Borgeaud, S., Irving, G., Lespiau, J., Sifre, L., and\n",
      "Jumper, J. Accelerating large language model decoding\n",
      "with speculative sampling. CoRR, abs/2302.01318, 2023.\n",
      "doi: 10.48550/ARXIV.2302.01318. URL https://\n",
      "doi.org/10.48550/arXiv.2302.01318.\n",
      "Chen, Y., Pan, X., Li, Y., Ding, B., and Zhou, J. Ee-llm:\n",
      "Large-scale training and inference of early-exit large lan-\n",
      "guage models with 3d parallelism, 2024.\n",
      "Choi, J., Wi, H., Kim, J., Shin, Y., Lee, K., Trask, N., and\n",
      "Park, N. Graph convolutions enrich the self-attention in\n",
      "transformers!, 2024.\n",
      "Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A.,\n",
      "Schoenick, C., and Tafjord, O. Think you have solved\n",
      "4\n",
      "Attention Is All You Need But You Don‚Äôt Need All Of It\n",
      "question answering? try arc, the ai2 reasoning challenge,\n",
      "2018.\n",
      "Din, A. Y., Karidi, T., Choshen, L., and Geva, M. Jump\n",
      "to conclusions: Short-cutting transformers with linear\n",
      "transformations. arXiv preprint arXiv:2303.09435, 2023.\n",
      "Dong, Y., Cordonnier, J.-B., and Loukas, A. Attention\n",
      "is not all you need: Pure attention loses rank doubly\n",
      "exponentially with depth, 2023.\n",
      "Dovonon, G. J.-S., Bronstein, M. M., and Kusner, M. J.\n",
      "Setting the record straight on transformer oversmoothing,\n",
      "2024.\n",
      "Elbayad, M., Gu, J., Grave, E., and Auli, M. Depth-adaptive\n",
      "transformer. In International Conference on Learning\n",
      "Representations, 2020. URL https://openreview.\n",
      "net/forum?id=SJg7KhVKPH.\n",
      "Elhoushi, M., Shrivastava, A., Liskovich, D., Hosmer, B.,\n",
      "Wasti, B., Lai, L., Mahmoud, A., Acun, B., Agarwal,\n",
      "S., Roman, A., et al. Layer skip: Enabling early exit\n",
      "inference and self-speculative decoding. arXiv preprint\n",
      "arXiv:2404.16710, 2024.\n",
      "Fan, A., Grave, E., and Joulin, A. Reducing transformer\n",
      "depth on demand with structured dropout, 2019.\n",
      "Fan, S., Jiang, X., Li, X., Meng, X., Han, P., Shang, S., Sun,\n",
      "A., Wang, Y., and Wang, Z. Not all layers of llms are\n",
      "necessary during inference, 2024.\n",
      "Fu, D. Y., Dao, T., Saab, K. K., Thomas, A. W., Rudra,\n",
      "A., and R¬¥e, C. Hungry hungry hippos: Towards lan-\n",
      "guage modeling with state space models. arXiv preprint\n",
      "arXiv:2212.14052, 2022.\n",
      "Gong, L., He, D., Li, Z., Qin, T., Wang, L., and Liu, T.\n",
      "Efficient training of bert by progressively stacking. In\n",
      "International conference on machine learning, pp. 2337‚Äì\n",
      "2346. PMLR, 2019.\n",
      "Graves, A. Adaptive computation time for recurrent neural\n",
      "networks, 2017.\n",
      "Gromov, A., Tirumala, K., Shapourian, H., Glorioso, P., and\n",
      "Roberts, D. A. The unreasonable ineffectiveness of the\n",
      "deeper layers, 2024.\n",
      "Gu, A. and Dao, T. Mamba: Linear-time sequence modeling\n",
      "with selective state spaces, 2023.\n",
      "Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M.,\n",
      "Song, D., and Steinhardt, J. Measuring massive multitask\n",
      "language understanding, 2021.\n",
      "Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C.,\n",
      "Chaplot, D. S., Casas, D. d. l., Bressand, F., Lengyel, G.,\n",
      "Lample, G., Saulnier, L., et al. Mistral 7b. arXiv preprint\n",
      "arXiv:2310.06825, 2023.\n",
      "Jiang, Y.-G., Cheng, C., Lin, H., and Fu, Y.\n",
      "Learning\n",
      "layer-skippable inference network. IEEE Transactions on\n",
      "Image Processing, 29:8747‚Äì8759, 2020. doi: 10.1109/\n",
      "TIP.2020.3018269.\n",
      "Kaddour, J., Harris, J., Mozes, M., Bradley, H., Raileanu,\n",
      "R., and McHardy, R. Challenges and applications of\n",
      "large language models. CoRR, abs/2307.10169, 2023a.\n",
      "doi: 10.48550/ARXIV.2307.10169. URL https://\n",
      "doi.org/10.48550/arXiv.2307.10169.\n",
      "Kaddour, J., Key, O., Nawrot, P., Minervini, P., and Kusner,\n",
      "M. J.\n",
      "No train no gain: Revisiting efficient training\n",
      "algorithms for transformer-based language models. In\n",
      "Oh, A., Naumann, T., Globerson, A., Saenko, K., Hardt,\n",
      "M., and Levine, S. (eds.), Advances in Neural Information\n",
      "Processing Systems 36: Annual Conference on Neural\n",
      "Information Processing Systems 2023, NeurIPS 2023,\n",
      "New Orleans, LA, USA, December 10 - 16, 2023, 2023b.\n",
      "Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu,\n",
      "C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient\n",
      "memory management for large language model serving\n",
      "with pagedattention. In Proceedings of the 29th Sym-\n",
      "posium on Operating Systems Principles, pp. 611‚Äì626,\n",
      "2023.\n",
      "Lin, S., Hilton, J., and Evans, O. Truthfulqa: Measuring\n",
      "how models mimic human falsehoods, 2022.\n",
      "Liu, Z., Wang, J., Dao, T., Zhou, T., Yuan, B., Song, Z.,\n",
      "Shrivastava, A., Zhang, C., Tian, Y., Re, C., and Chen,\n",
      "B. Deja vu: Contextual sparsity for efficient LLMs at\n",
      "inference time. In Krause, A., Brunskill, E., Cho, K.,\n",
      "Engelhardt, B., Sabato, S., and Scarlett, J. (eds.), Pro-\n",
      "ceedings of the 40th International Conference on Ma-\n",
      "chine Learning, volume 202 of Proceedings of Machine\n",
      "Learning Research, pp. 22137‚Äì22176. PMLR, 23‚Äì29 Jul\n",
      "2023. URL https://proceedings.mlr.press/\n",
      "v202/liu23am.html.\n",
      "Men, X., Xu, M., Zhang, Q., Wang, B., Lin, H., Lu, Y., Han,\n",
      "X., and Chen, W. Shortgpt: Layers in large language\n",
      "models are more redundant than you expect, 2024. URL\n",
      "https://arxiv.org/abs/2403.03853.\n",
      "Nawrot, P., ≈Åa¬¥ncucki, A., Chochowski, M., Tarjan, D., and\n",
      "Ponti, E. M. Dynamic memory compression: Retrofitting\n",
      "llms for accelerated inference, 2024.\n",
      "Patterson, D. A., Gonzalez, J., Le, Q. V., Liang, C., Munguia,\n",
      "L., Rothchild, D., So, D. R., Texier, M., and Dean, J. Car-\n",
      "bon emissions and large neural network training. CoRR,\n",
      "5\n",
      "Attention Is All You Need But You Don‚Äôt Need All Of It\n",
      "abs/2104.10350, 2021. URL https://arxiv.org/\n",
      "abs/2104.10350.\n",
      "Peng, B., Alcaide, E., Anthony, Q., Albalak, A., Arcadinho,\n",
      "S., Cao, H., Cheng, X., Chung, M., Grella, M., GV, K. K.,\n",
      "et al. Rwkv: Reinventing rnns for the transformer era.\n",
      "arXiv preprint arXiv:2305.13048, 2023.\n",
      "Reid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lilli-\n",
      "crap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat,\n",
      "O., Schrittwieser, J., et al. Gemini 1.5: Unlocking multi-\n",
      "modal understanding across millions of tokens of context.\n",
      "arXiv preprint arXiv:2403.05530, 2024.\n",
      "Sajjad, H., Dalvi, F., Durrani, N., and Nakov, P. On the\n",
      "effect of dropping layers of pre-trained transformer mod-\n",
      "els.\n",
      "Computer Speech & Language, 77:101429, jan\n",
      "2023. doi: 10.1016/j.csl.2022.101429. URL https:\n",
      "//doi.org/10.1016%2Fj.csl.2022.101429.\n",
      "Schuster, T., Fisch, A., Gupta, J., Dehghani, M., Bahri, D.,\n",
      "Tran, V., Tay, Y., and Metzler, D. Confident adaptive\n",
      "language modeling. Advances in Neural Information\n",
      "Processing Systems, 35:17456‚Äì17472, 2022.\n",
      "Teerapittayanon, S., McDanel, B., and Kung, H. T.\n",
      "Branchynet: Fast inference via early exiting from deep\n",
      "neural networks, 2017.\n",
      "Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux,\n",
      "M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E.,\n",
      "Azhar, F., Rodriguez, A., Joulin, A., Grave, E., and Lam-\n",
      "ple, G. Llama: Open and efficient foundation language\n",
      "models, 2023a.\n",
      "Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi,\n",
      "A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P.,\n",
      "Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen,\n",
      "M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W.,\n",
      "Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn,\n",
      "A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\n",
      "V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S.,\n",
      "Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y.,\n",
      "Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog,\n",
      "I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi,\n",
      "K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R.,\n",
      "Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X.,\n",
      "Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur,\n",
      "M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S.,\n",
      "and Scialom, T. Llama 2: Open foundation and fine-tuned\n",
      "chat models, 2023b.\n",
      "Wang, J., Chen, K., Chen, G., Shou, L., and McAuley, J.\n",
      "Skipbert: Efficient inference with shallow layer skipping.\n",
      "In Proceedings of the 60th Annual Meeting of the Asso-\n",
      "ciation for Computational Linguistics (Volume 1: Long\n",
      "Papers), pp. 7287‚Äì7301, 2022a.\n",
      "Wang, P., Zheng, W., Chen, T., and Wang, Z.\n",
      "Anti-\n",
      "oversmoothing in deep vision transformers via the fourier\n",
      "domain analysis:\n",
      "From theory to practice.\n",
      "In In-\n",
      "ternational Conference on Learning Representations,\n",
      "2022b. URL https://openreview.net/forum?\n",
      "id=O476oWmiNNp.\n",
      "Wu, H. and Tu, K. Layer-condensed kv cache for efficient\n",
      "inference of large language models, 2024.\n",
      "Xia, H., Yang, Z., Dong, Q., Wang, P., Li, Y., Ge, T., Liu, T.,\n",
      "Li, W., and Sui, Z. Unlocking efficiency in large language\n",
      "model inference: A comprehensive survey of speculative\n",
      "decoding. arXiv preprint arXiv:2401.07851, 2024.\n",
      "Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y.\n",
      "Hellaswag: Can a machine really finish your sentence?,\n",
      "2019.\n",
      "Zhai, S., Likhomanenko, T., Littwin, E., Busbridge, D.,\n",
      "Ramapuram, J., Zhang, Y., Gu, J., and Susskind, J. Sta-\n",
      "bilizing transformer training by preventing attention en-\n",
      "tropy collapse, 2023.\n",
      "Zhang, M. and He, Y. Accelerating training of transformer-\n",
      "based language models with progressive layer dropping.\n",
      "Advances in neural information processing systems, 33:\n",
      "14011‚Äì14023, 2020.\n",
      "6\n",
      "\n",
      "üìò Paper 2:\n",
      "{'Published': '2025-10-21', 'Title': 'Some Attention is All You Need for Retrieval', 'Authors': 'Felix Michalak, Steven Abreu', 'Summary': 'We demonstrate complete functional segregation in hybrid SSM-Transformer\\narchitectures: retrieval depends exclusively on self-attention layers. Across\\nRecurrentGemma-2B/9B and Jamba-Mini-1.6, attention ablation causes catastrophic\\nretrieval failure (0% accuracy), while SSM layers show no compensatory\\nmechanisms even with improved prompting. Conversely, sparsifying attention to\\njust 15% of heads maintains near-perfect retrieval while preserving 84% MMLU\\nperformance, suggesting self-attention specializes primarily for retrieval\\ntasks. We identify precise mechanistic requirements for retrieval: needle\\ntokens must be exposed during generation and sufficient context must be\\navailable during prefill or generation. This strict functional specialization\\nchallenges assumptions about redundancy in hybrid architectures and suggests\\nthese models operate as specialized modules rather than integrated systems,\\nwith immediate implications for architecture optimization and interpretability.'}\n",
      "\n",
      "üìò Title:\n",
      "Some Attention is All You Need for Retrieval\n",
      "\n",
      "üìò Published::\n",
      "2025-10-21\n",
      "\n",
      "üìò Authors::\n",
      "Felix Michalak, Steven Abreu\n",
      "\n",
      "Some Attention is All You Need for Retrieval\n",
      "Felix Michalak‚àó\n",
      "University of Groningen\n",
      "The Netherlands\n",
      "k.f.michalak@student.rug.nl\n",
      "Steven Abreu\n",
      "University of Groningen\n",
      "The Netherlands\n",
      "s.abreu@rug.nl\n",
      "Abstract\n",
      "We demonstrate complete functional segregation in hybrid SSM-Transformer\n",
      "architectures: retrieval depends exclusively on self-attention layers.\n",
      "Across\n",
      "RecurrentGemma-2B/9B and Jamba-Mini-1.6, attention ablation causes catas-\n",
      "trophic retrieval failure (0% accuracy), while SSM layers show no compensatory\n",
      "mechanisms even with improved prompting. Conversely, sparsifying attention to\n",
      "just 15% of heads maintains near-perfect retrieval while preserving 84% MMLU\n",
      "performance, suggesting self-attention specializes primarily for retrieval tasks. We\n",
      "identify precise mechanistic requirements for retrieval: needle tokens must be\n",
      "exposed during generation and sufficient context must be available during pre-\n",
      "fill or generation. This strict functional specialization challenges assumptions\n",
      "about redundancy in hybrid architectures and suggests these models operate as\n",
      "specialized modules rather than integrated systems, with immediate implications\n",
      "for architecture optimization and interpretability.\n",
      "1\n",
      "Introduction\n",
      "Hybrid SSM-Transformer architectures combine linearly scaling state-space models with quadratically\n",
      "scaling attention mechanisms, promising efficient processing of long sequences without sacrificing\n",
      "performance (Botev et al., 2024; Lieber et al., 2024). While SSMs provide computational efficiency\n",
      "through recurrent state evolution, they exhibit ‚Äúfuzzy memory‚Äù‚Äìcapturing patterns but struggling with\n",
      "precise retrieval (Waleffe et al., 2024). Attention mechanisms excel at exact retrieval through direct\n",
      "token comparisons (Olsson et al., 2022), motivating architectures that interleave both components\n",
      "layer-wise. Despite their practical success, a fundamental question remains: do these architectural\n",
      "components develop overlapping capabilities for robustness, or does training induce strict functional\n",
      "segregation? Understanding this functional organization is essential for both mechanistic inter-\n",
      "pretability and practical model design. Overlapping capabilities would suggest redundancy, enabling\n",
      "architectural simplification, while strict specialization would indicate task-specific computational\n",
      "delegation.\n",
      "This distinction has profound implications for interpretability research. Functional segregation would\n",
      "enable targeted analysis of specific capabilities by studying the corresponding specialized components,\n",
      "facilitating a more precise mechanistic understanding. Conversely, overlapping capabilities would\n",
      "require analyzing complex interactions between components, complicating interpretability efforts but\n",
      "potentially revealing emergent computational strategies.\n",
      "From a practical design perspective, understanding component specialization directly informs archi-\n",
      "tecture optimization. If attention layers specialize exclusively for retrieval while SSM layers handle\n",
      "other language modeling functions, this knowledge enables targeted architectural modifications‚Äì\n",
      "such as specialized retrieval modules or attention sparsification strategies‚Äìthat preserve essential\n",
      "capabilities while reducing computational overhead.\n",
      "‚àóCorrespondence: For inquiries, please reach out to felix[at]michalax[dot]de\n",
      "39th Conference on Neural Information Processing Systems (NeurIPS 2025) Workshop: Mechanistic Inter-\n",
      "pretability.\n",
      "arXiv:2510.19861v1  [cs.LG]  21 Oct 2025\n",
      "We present a mechanistic analysis that shines light on this question. Through systematic ablation\n",
      "studies across RecurrentGemma-2B/9B and Jamba-Mini-1.6, we uncover complete functional segre-\n",
      "gation: retrieval is exclusive to self-attention layers, while SSM components contribute nothing to\n",
      "this function. This finding represents a fundamental insight into how hybrid architectures organize\n",
      "their computational capabilities during training.\n",
      "Our investigation systematically tests three falsifiable hypotheses that probe the mechanistic under-\n",
      "pinnings of hybrid architectures:\n",
      "H1: Functional Exclusivity: Retrieval in hybrid architectures depends exclusively on self-\n",
      "attention layers, with SSM layers contributing no retrieval capability. Furthermore, retrieval\n",
      "ability in the SSM layers cannot be recovered through prompting strategies designed to\n",
      "improve retrieval in SSMs.\n",
      "H2: Retrieval Specialization: Self-attention layers specialize primarily for retrieval, such that\n",
      "sparsification preserving minimal retrieval-critical heads maintains general language mod-\n",
      "eling capabilities while complete ablation causes retrieval-specific failure without broad\n",
      "performance degradation.\n",
      "H3: Retrieval Conditions: Successful retrieval depends on specific mechanistic requirements:\n",
      "needle token exposure during the generation phase and sufficient contextual information\n",
      "available during either prefill or generation phases, where precise attention weight patterns\n",
      "constitute necessary components.\n",
      "These hypotheses directly address the central research gap by testing whether hybrid architectures ex-\n",
      "hibit functional redundancy or strict specialization. The systematic testing of these hypotheses through\n",
      "controlled ablation studies across multiple models provides definitive evidence for understanding\n",
      "hybrid model organization.\n",
      "To rigorously test these hypotheses, we employ entropy-based attention sparsification inspired by Liu\n",
      "et al. (2023) to progressively ablate attention mechanisms while monitoring retrieval performance on\n",
      "the Needle-In-A-Haystack (NIAH) benchmark (Bai et al., 2024). We complement this with Just Read\n",
      "Twice (JRT) prompting (Arora et al., 2024) to attempt recovery of SSM-based retrieval capabilities.\n",
      "Additionally, we assess broader capabilities using GLUE (Wang et al., 2019) and MMLU (Hendrycks\n",
      "et al., 2020) benchmarks to ensure that observed retrieval-specific effects do not indicate general\n",
      "performance degradation.\n",
      "Our analysis reveals three critical insights: (1) Across model scales and architectural variants, as few\n",
      "as 15% of attention heads maintain near-perfect retrieval while complete ablation causes catastrophic\n",
      "failure, demonstrating that SSM layers do not contribute to retrieval. (2) This sparsification preserves\n",
      "84% performance on MMLU, isolating retrieval without broadly degrading model performance,\n",
      "thereby revealing opportunities for more effective model design. (3) Successful retrieval requires\n",
      "specific mechanistic conditions‚Äìneedle token exposure during generation plus sufficient contextual\n",
      "information during prefill and generation phases‚Äìestablishing precise requirements for the attention-\n",
      "based retrieval mechanism.\n",
      "These findings provide the first definitive evidence of complete functional segregation in hybrid\n",
      "architectures, revealing that training induces strict specialization. This understanding enables targeted\n",
      "optimization strategies, more precise interpretability research, and informed design of future hybrid\n",
      "architectures that leverage the distinct strengths of each component type.\n",
      "2\n",
      "Methods\n",
      "2.1\n",
      "Models and Experimental Setup\n",
      "We investigate functional segregation in hybrid SSM-Transformer architectures through systematic\n",
      "ablation studies across three models: RecurrentGemma-2B (RG-2B), RecurrentGemma-9B (RG-9B),\n",
      "and Jamba-Mini-1.6 (Jamba). These models represent diverse architectural approaches to hybrid\n",
      "design, varying in size, attention mechanism implementation, and component organization.\n",
      "RG-2B contains 2B parameters across 26 layers organized as eight repetitions of the pattern \"2√ó\n",
      "SSM, 1√ó Attention\" followed by two SSM layers, with 10 attention heads per attention layer. RG-9B\n",
      "extends this architecture to 9B parameters across 38 layers (12 pattern repetitions) with 16 heads\n",
      "2\n",
      "per attention layer. Both RecurrentGemma models employ sliding window attention with a 2048-\n",
      "token window. Jamba represents a distinct architectural approach with 51B total parameters (12B\n",
      "active during inference) organized across 32 layers following the pattern \"3√ó SSM, 1√ó Attention,\n",
      "4√ó SSM\". Unlike RecurrentGemma, Jamba utilizes global attention with 32 heads per layer and\n",
      "incorporates Mixture of Experts (MoE) layers that substitute standard MLPs in every other layer. This\n",
      "architectural diversity enables testing whether functional segregation emerges consistently across\n",
      "different hybrid designs. All experiments employ a unified experimental framework implemented\n",
      "through ManipuLatte2, our custom library providing consistent interfaces for attention manipulation\n",
      "across architectures. Models were tested with batch size of one, with prompt lengths up to 4096\n",
      "tokens distributed across ten lengths and ten needle depths (100 prompts total), following the protocol\n",
      "established by Zani et al. (2025). Complete model specifications and computational resources are\n",
      "detailed in Appendix A.\n",
      "2.2\n",
      "Attention Sparsification Method\n",
      "Inspired by the contextual sparsity introduced by Liu et al. (2023), we employ a simpler entropy-\n",
      "based top-k sparsification to systematically ablate attention mechanisms while preserving the most\n",
      "informative heads. For each attention head, we calculate the entropy of attention weight distributions\n",
      "as:\n",
      "H(A) = ‚àí\n",
      "X\n",
      "t\n",
      "at √ó log2(at)\n",
      "(1)\n",
      "where A represents the attention weight vector and at denotes the attention weight for token t.\n",
      "Lower entropy indicates more focused attention patterns, suggesting greater information content and\n",
      "specificity in the attention mechanism.\n",
      "During sparsification, we retain only the k attention heads with the lowest entropy across all layers,\n",
      "ablating the remaining heads by setting their corresponding values in the attention output to zero.\n",
      "This approach preserves heads that exhibit the most decisive attention patterns while removing those\n",
      "with more uniform (higher entropy) distributions. We test values of k ‚àà[0, N] where N equals\n",
      "the total number of heads per layer (10 for RG-2B, 16 for RG-9B, 32 for Jamba), with k = 0\n",
      "representing complete attention ablation and k = N representing the unmodified model. Unless\n",
      "otherwise specified, we apply sparsity only during generation and keep k = N fixed during prefill.\n",
      "See Section 2.5 for more details.\n",
      "2.3\n",
      "Attempting to Improve SSM Retrieval with Just Read Twice\n",
      "To test whether SSM layers can recover retrieval capabilities when attention is ablated, we apply Just\n",
      "Read Twice (JRT) prompting (Arora et al., 2024) for severely sparsified models (k ‚àà{0, 1, 2}). JRT\n",
      "repeats the context and question to potentially activate latent retrieval mechanisms in SSM layers,\n",
      "providing a stringent test of functional exclusivity.\n",
      "2.4\n",
      "Evaluation Benchmarks\n",
      "We employ three complementary benchmarks to isolate retrieval-specific effects from general per-\n",
      "formance degradation. The Needle-In-A-Haystack (NIAH) benchmark (Bai et al., 2024) directly\n",
      "measures retrieval capabilities by requiring models to extract specific information (the ‚Äúneedle‚Äù) from\n",
      "contexts filled with irrelevant text. Complete NIAH specifications and scoring rubrics are provided in\n",
      "Appendix B.\n",
      "To assess broader language modeling capabilities, we employ GLUE (Wang et al., 2019) and MMLU\n",
      "(Hendrycks et al., 2020) benchmarks using the LM-Evaluation-Harness (Gao et al., 2024). GLUE\n",
      "evaluates fundamental natural language understanding through nine tasks, including sentiment\n",
      "analysis and textual entailment (zero-shot configuration). MMLU tests advanced reasoning across 57\n",
      "subjects spanning STEM, humanities, and social sciences (five-shot configuration). Both benchmarks\n",
      "evaluate using the log-likelihood of multiple-choice options after the prefill stage, enabling assessment\n",
      "of how attention sparsification affects general capabilities versus retrieval-specific functions.\n",
      "2Available at https://github.com/lamalunderscore/manipulatte\n",
      "3\n",
      "2.5\n",
      "Attention Manipulation Techniques\n",
      "To understand the mechanistic requirements for successful retrieval, we implement four targeted\n",
      "attention weight manipulation techniques applied during different inference phases. These manipu-\n",
      "lations isolate specific components of the attention mechanism to identify necessary conditions for\n",
      "retrieval.\n",
      "The Only manipulation nullifies attention weights for all tokens except needle tokens, which retain\n",
      "their original values, isolating the direct attention to retrieval targets. The Omit manipulation inverts\n",
      "this by nullifying only needle token weights while preserving all others, testing whether contextual\n",
      "information alone suffices for retrieval. The Binary manipulation extends Only by assigning uniform\n",
      "average weights across needle tokens, testing whether precise weight values or merely token exposure\n",
      "drives retrieval. The Null manipulation ablates all attention weights, equivalent to k = 0 sparsification.\n",
      "See Figure 1 for a visual representation.\n",
      "0\n",
      "T\n",
      "Token\n",
      "Attention weight\n",
      "(a) Only\n",
      "0\n",
      "T\n",
      "Token\n",
      "Attention weight\n",
      "(b) Omit\n",
      "0\n",
      "T\n",
      "Token\n",
      "Attention weight\n",
      "(c) Binary\n",
      "0\n",
      "T\n",
      "Token\n",
      "Attention weight\n",
      "(d) Null\n",
      "Figure 1: Simplified representation of manipulation methods. Original attention weights (dotted\n",
      "blue), modified weights (solid blue), and needle tokens (yellow highlight).\n",
      "We test all combinations of prefill and generation manipulations (denoted as \"Generation-Prefill\",\n",
      "e.g., \"Omit-Null\" applies Omit during generation and Null during prefill), excluding Null during\n",
      "generation alone as this equals complete sparsification. This systematic manipulation reveals the\n",
      "precise conditions required for the attention-based retrieval mechanism to function successfully.\n",
      "3\n",
      "Retrieval Depends Exclusively on Self-Attention\n",
      "Testing H1 (functional exclusivity) requires demonstrating that retrieval depends exclusively on\n",
      "self-attention layers with no SSM contribution, even under prompting strategies designed to enhance\n",
      "SSM performance. We systematically vary attention sparsification levels while monitoring retrieval\n",
      "performance, complemented by Just Read Twice (JRT) prompting (Arora et al., 2024) to attempt\n",
      "recovery of potential latent SSM retrieval capabilities.\n",
      "3.1\n",
      "Results\n",
      "As seen in Figure 2, all three models showed distinct but similar behaviors for sparsification. We\n",
      "refered to the model versions that are only sparsified during the generation stage as generation\n",
      "versions, and to the versions that are sparsified during both stages as prefill versions.\n",
      "All models maintained high accuracy until model-specific thresholds, then showed gradual degrada-\n",
      "tion before catastrophic failure. Jamba sustained 100% accuracy until k = 27, then declined to k = 6\n",
      "where it sharply dropped to zero. RG-9B degraded from k = 16 to k = 3 before failing completely\n",
      "at k = 2. RG-2B‚Äôs generation version remained stable until k = 1 where it collapsed, while its\n",
      "prefill version immediately dropped to 20% accuracy. Critically, all models exhibited a final tipping\n",
      "point (k ‚àà{1, 2, 3, 4} for generation versions) where accuracy abruptly fell to zero‚Äìthis threshold\n",
      "correlated with attention head count (32, 16, and 10 heads respectively).\n",
      "Following Figure 3, applying JRT did not recover retrieval capabilities at low k. See Appendix C for\n",
      "a comparison between retrieval maps for standard and JRT prompting.\n",
      "4\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "RG9B - Generation\n",
      "RG9B - Prefill\n",
      "RG2B - Generation\n",
      "RG2B - Prefill\n",
      "Jamba - Generation\n",
      "Jamba - Prefill\n",
      "Accuracy~K on NIAH for all models\n",
      "K\n",
      "Accuracy in %\n",
      "Figure 2: Accuracy as a function of k in top-k sparsification on standard NIAH for generation and\n",
      "prefill versions. Accuracy was approximated by the average score across all 100 prompts, relative\n",
      "to the maximum score (5). Note that scoring was neither linear nor continuous (see Table 2 for\n",
      "interpretation guidance). Read from right to left for increasing sparsity.\n",
      "3.2\n",
      "Discussion\n",
      "Retrieval failed for all models at k = 0, which confirmed that the findings made by Zani et al. (2025)\n",
      "also hold for other hybrid LLMs. However, all three models showed slightly different tipping points in\n",
      "generation versions, after which the accuracy started to decrease drastically: k = 4 for Jamba, k = 2\n",
      "for RG-9B, and k = 1 for RG-2B. Still, the overall behavior is similar across all three models. Note\n",
      "that our results showed the tipping point for RG-2B at k = 1, whereas Zani et al. (2025) identified\n",
      "this tipping point to be around k = 2. This difference can be attributed to our improved scoring\n",
      "method and prompt structure.\n",
      "Regarding the prefill versions, the difference in behaviors was striking, especially prominent in the\n",
      "behavior of RG-2B. While the RG-2B prefill version failed even at low sparsification levels, the\n",
      "Jamba prefill version performed almost identically to its generation version. It seems as though the\n",
      "effect size of prefill sparsification reduces with increasing model size.\n",
      "It is important to consider the impact of sparsification on the internals of a model. Any manipulation\n",
      "of the computations during a forward pass will alter the residual stream and therefore yield, compared\n",
      "to the original activations, an imperfect version of all downstream activations. This means that\n",
      "every layer activation after the first manipulation is imperfect, and every further manipulation likely\n",
      "increases the magnitude of the effect. This also means that every query-, key- and value-projection is\n",
      "imperfect. Since the RG models used a cache for the values of key- and value-projections (kv-cache)\n",
      "to optimize inference time, the query-projection will use the faulty cached key- and value-projections\n",
      "for every subsequent forward pass during that inference run, increasing the impact of the manipulation.\n",
      "Especially important for this cache is the prefill stage, as the cache gets filled with all projections of\n",
      "the prompt tokens. Jamba was not set up to use this caching mechanism. Hence, we would expect the\n",
      "RG models to be impacted more by prefill sparsification.\n",
      "Since all models failed to retrieve the needle at k = 0, and applying JRT did not recover this\n",
      "retrieval capability, this provides further strong evidence that retrieval in hybrid LLMs is exclusively\n",
      "implemented through self-attention layers. This also further solidifies the hypothesis that during\n",
      "training, only self-attention layers learn the retrieval function, so much so that SSM layers do not\n",
      "even develop the common fuzzy memory (Waleffe et al., 2024).\n",
      "Future work could compare state transition matrices between pure and hybrid SSM variants to\n",
      "uncover how retrieval mechanisms differ when attention is available versus absent, potentially\n",
      "revealing fundamental SSM retrieval strategies.\n",
      "4\n",
      "Self-Attention Specializes for Retrieval\n",
      "Testing H2 (retrieval specialization) requires demonstrating that self-attention layers specialize\n",
      "primarily for retrieval, such that sparsification preserving minimal retrieval-critical heads maintains\n",
      "5\n",
      "general capabilities. We evaluate this through systematic benchmarking on GLUE and MMLU at\n",
      "different sparsification levels.\n",
      "We test three configurations on Jamba: base (unmodified), optimally sparsified (k = 5, the minimal\n",
      "prefill sparsification maintaining near-perfect retrieval from Section 3), and fully ablated (k = 0,\n",
      "complete attention removal). This comparison isolates the impact of attention sparsification on\n",
      "general versus retrieval-specific capabilities. We apply sparsity during prefill, as the benchmark\n",
      "answers are evaluated by log-likelihood without generation.\n",
      "4.1\n",
      "Results\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "RG9B\n",
      "RG9B - JRT\n",
      "RG2B\n",
      "RG2B - JRT\n",
      "Jamba\n",
      "Jamba - JRT\n",
      "Accuracy~K on NIAH - JRT\n",
      "K\n",
      "Accuracy in %\n",
      "MMLU\n",
      "GLUE\n",
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "Ablated\n",
      "Sparse\n",
      "Base\n",
      "Benchmark Scores, Ablated/Sparse/Base Jamba-Mini-1.6\n",
      "Benchmark\n",
      "Score in %\n",
      "Random Guessing Baseline\n",
      "Figure 3: Left: Accuracy as a function of k in top-k sparsification on NIAH with and without JRT\n",
      "applied, for all models in the generation version. Accuracy was approximated by the average score\n",
      "across all 100 prompts, relative to the maximum score (5). Note that scoring was neither linear nor\n",
      "continuous (see Table 2 for interpretation guidance). Read from right to left for increasing sparsity.\n",
      "Right: Scores for Jamba-Mini-1.6, in an ablated configuration (k = 0), a sparse configuration\n",
      "(k = 5), and the base configuration, on GLUE and MMLU.\n",
      "Figure 3 shows the average achieved scores per benchmark, per model configuration. See Appendix\n",
      "D for subtask results.\n",
      "On GLUE, the base configuration of Jamba performed best, but the ablated and sparsified versions\n",
      "only scored slightly lower (6.6 and 9.1 points, respectively). Although only marginally, the ablated\n",
      "version scored higher than the sparse version (2.8 points). On MMLU, the base configuration\n",
      "performed best again. The sparse version only scored six points below the base configuration,\n",
      "whereas the ablated version scored around 42 points below the base configuration.\n",
      "4.2\n",
      "Discussion\n",
      "The results provide evidence that the retrieval capability of self-attention layers can be isolated\n",
      "through sparsification. However, the effect of sparsification depends significantly on the complexity\n",
      "and nature of the evaluated task.\n",
      "The big difference in the effect of sparsification and ablation between MMLU and GLUE highlights\n",
      "the necessity for retrieval capabilities for more difficult language modeling tasks, whereas general\n",
      "language capabilities do not depend on retrieval to such a degree.\n",
      "Still, the sparsified configuration performing worse than the ablated configuration in GLUE hints at a\n",
      "suboptimal sparsification method. Ablating the wrong attention heads can lead to broken retrieval\n",
      "functions, effectively introducing noise or even facilitating a misleading information flow. Instead of\n",
      "investigating attention heads behaviorally, future iterations could structurally investigate attention\n",
      "heads through an offline analysis that examines the weights of different circuits in the self-attention\n",
      "6\n",
      "architecture, as performed by Olsson et al. (2022). Such an analysis could provide insights into which\n",
      "heads are implementing which kind of capabilities and functions before running inference. This way,\n",
      "instead of dynamically choosing the attention heads to ablate at run time, a model could be sparsified\n",
      "statically, based on the characteristics of different heads, and with more care.\n",
      "5\n",
      "Mechanistic Requirements for Retrieval\n",
      "Testing H3 (retrieval conditions) requires identifying the specific mechanistic requirements for\n",
      "successful retrieval. We systematically manipulate attention weights to isolate necessary conditions:\n",
      "needle token exposure and contextual information availability during different inference phases.\n",
      "We apply the four manipulation techniques described in Section 2.5 (Only, Omit, Binary, Null) to RG-\n",
      "2B without sparsification (k = N), testing all combinations of prefill and generation manipulations.\n",
      "This approach reveals which attention components are mechanistically necessary for retrieval.\n",
      "5.1\n",
      "Results\n",
      "Generation\n",
      "Keep\n",
      "Omit\n",
      "Only\n",
      "Binary\n",
      "Prefill\n",
      "Keep\n",
      "50.3%\n",
      "3.7%\n",
      "85.7%\n",
      "3.7%\n",
      "Omit\n",
      "63.0%\n",
      "0.0%\n",
      "53.0%\n",
      "0.0%\n",
      "Only\n",
      "70.6%\n",
      "0.1%\n",
      "18.2%\n",
      "0.1%\n",
      "Binary\n",
      "0.0%\n",
      "0.0%\n",
      "0.0%\n",
      "0.0%\n",
      "Null\n",
      "68.8%\n",
      "0.0%\n",
      "17.0%\n",
      "0.0%\n",
      "Table 1: NIAH results for all tested manipulation combinations on RecurrentGemma-2B.Marked as\n",
      "overall best, second best and third best\n",
      ".\n",
      "Table 1 gives an overview of the accuracies scored in NIAH for different combinations of prefill\n",
      "and generation manipulation. Omitting the needle tokens during generation led to a drastic drop in\n",
      "accuracy, with Omit-Keep reaching 3.7% accuracy, and all other combinations scoring zero or close\n",
      "to zero percent accuracy. Applying the Binary method during generation yielded the same accuracies\n",
      "as applying the Omit method; all combinations applying Binary during prefill scored zero percent\n",
      "accuracy. The combinations Keep-Omit, Keep-Only, and Keep-Null all improved the accuracy relative\n",
      "to the base configuration, with 63.0%, 70.6%, and 68.8%, respectively. When only keeping the needle\n",
      "tokens during prefill, not manipulating the prefill, or omitting the needle tokens also improved the\n",
      "scores relative to the base configuration, with 85.7% and 53.0%, respectively. Only-Keep reached\n",
      "the highest score across all combinations. Note that every sparsification combination that did not\n",
      "severely reduce retrieval capabilities yielded better scores than the base model (Keep-Keep).\n",
      "For a complete overview of all retrieval maps, see Appendix E.\n",
      "5.2\n",
      "Discussion\n",
      "The results suggest that successful retrieval necessitates, firstly, exposure of the needle tokens during\n",
      "generation, and secondly, exposure of tokens that provide sufficient context either during prefill or\n",
      "during generation. The first point is obvious from the results when applying Omit during generation.\n",
      "Every combination, including Omit-Keep, fails retrieval. To deduce the second point, the key\n",
      "combinations to look at are Only-Omit and Only-Only, as well as Keep-Omit, Keep-Only, and\n",
      "Keep-Null. Only-Omit shows that, even when only exposing the needle tokens during generation,\n",
      "only exposing the structure around the needle during prefill suffices to facilitate successful (above\n",
      "baseline) retrieval capabilities; when not exposing the structure during prefill, like in Only-Only,\n",
      "retrieval fails. The other three combinations, especially Keep-Null, show that the tokens exposed\n",
      "during prefill are irrelevant, likely because enough tokens are exposed during generation.\n",
      "7\n",
      "250\n",
      "677\n",
      "1105\n",
      "1532\n",
      "1959\n",
      "2387\n",
      "2814\n",
      "3241\n",
      "3669\n",
      "4096\n",
      "Token Limit\n",
      "0.0\n",
      "11.0\n",
      "22.0\n",
      "33.0\n",
      "44.0\n",
      "56.0\n",
      "67.0\n",
      "78.0\n",
      "89.0\n",
      "100.0\n",
      "Depth Percent\n",
      "(a) Keep-Omit\n",
      "250\n",
      "677\n",
      "1105\n",
      "1532\n",
      "1959\n",
      "2387\n",
      "2814\n",
      "3241\n",
      "3669\n",
      "4096\n",
      "Token Limit\n",
      "0.0\n",
      "11.0\n",
      "22.0\n",
      "33.0\n",
      "44.0\n",
      "56.0\n",
      "67.0\n",
      "78.0\n",
      "89.0\n",
      "100.0\n",
      "(b) Omit-Keep\n",
      "250\n",
      "677\n",
      "1105\n",
      "1532\n",
      "1959\n",
      "2387\n",
      "2814\n",
      "3241\n",
      "3669\n",
      "4096\n",
      "Token Limit\n",
      "0.0\n",
      "11.0\n",
      "22.0\n",
      "33.0\n",
      "44.0\n",
      "56.0\n",
      "67.0\n",
      "78.0\n",
      "89.0\n",
      "100.0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Score (0 = Failure, 5 = Perfect)\n",
      "(c) Keep-Keep (Base)\n",
      "Figure 4: Retrieval maps for RG-2B on NIAH with the manipulations Keep-Omit, Omit-Keep, and\n",
      "Keep-Keep applied. The x-axis shows the used prompt length, the y-axis the depth of the needle, 0%\n",
      "being the very end of the prompt and 100% being the very beginning of the prompt.\n",
      "Additionally, the low accuracy scores when applying Binary showed that the values of the attention\n",
      "weights are important for successful retrieval. It is not enough to expose necessary tokens more than\n",
      "unnecessary ones, but the exact structure of and relations between the attention weights matter.\n",
      "Investigating the retrieval maps of different combinations revealed further details, refining the previous\n",
      "hypothesis. The retrieval maps for Keep-Omit (Figure 4a) and Omit-Keep (Figure 4b) complement\n",
      "each other to make up an improved version of Keep-Keep (Figure 4c). This means that Omit-Keep,\n",
      "which was previously labeled as a retrieval failure, was not a failure but only a partial success.\n",
      "The partial retrieval success in Figure 4b aligns with RecurrentGemma‚Äôs 2048-token sliding window‚Äì\n",
      "successful retrievals occur only when needles fall within this window during generation. Retrieval\n",
      "beyond the window requires prefill exposure, suggesting the kv-cache preserves implicit retrieval\n",
      "information even when tokens exit the active attention window. See Figure 10 for complete retrieval\n",
      "maps.\n",
      "Since the Binary method yielded such detrimental results, we examined the outputs of corresponding\n",
      "combinations. The outputs showed that when Binary was applied during prefill, the generated tokens\n",
      "were mostly nonsensical. When Binary was applied during generation, the generated tokens made\n",
      "sense, but were off topic. See Appendix F for a selection of pearls of wisdom generated by confused\n",
      "hybrid models.\n",
      "Applying Binary during prefill likely has a similar effect on the kv-cache as sparsifying during the\n",
      "prefill stage (see Section 3.2), producing nonsensical tokens during generation. This would also\n",
      "explain why applying Binary only during generation did not result in such catastrophic failures, as\n",
      "the kv-cache was prefilled with unmanipulated keys and values.\n",
      "Overall, the Binary method was not thoroughly investigated before employing it, but rather it was\n",
      "implemented in a way that most efficiently applied the idea of unifying attention weights across\n",
      "a range of tokens. For example, upon examining attention weights for the first activations in the\n",
      "generation stage, it became obvious that the range of the needle tokens always started with a large\n",
      "peak in the weights on the exact token that would have to be predicted (also visible in Figure 1). The\n",
      "behavior of the attention weights throughout the generation process was not further investigated. This\n",
      "peak likely moves across the range of needle tokens, always being located on the token corresponding\n",
      "to the correct prediction. A behavior like this would mean that the method used to apply the idea\n",
      "of unifying and binarizing attention weights was inadequate. This limitation could be overcome in\n",
      "future research.\n",
      "6\n",
      "Conclusion\n",
      "We established complete functional segregation in hybrid SSM-Transformer architectures: retrieval\n",
      "depends exclusively on self-attention layers while SSMs handle general language capabilities. Across\n",
      "RecurrentGemma and Jamba models, attention ablation caused catastrophic retrieval failure (0%\n",
      "accuracy) with no SSM compensation, even under specialized prompting. Yet sparsifying to just 15%\n",
      "of attention heads preserved near-perfect retrieval and 84% MMLU performance, revealing precise\n",
      "8\n",
      "functional specialization. Our entropy-based sparsification method showed suboptimal performance\n",
      "in some cases, indicating the need for more sophisticated approaches such as structural circuit analysis\n",
      "of the self-attention mechanism, as was performed by Olsson et al. (2022).\n",
      "This functional segregation in hybrid models enables immediate practical optimizations‚Äìtargeted\n",
      "sparsification, specialized retrieval modules, and component-specific deployment strategies. Future\n",
      "work may explore whether this specialization emerges from architectural constraints or training\n",
      "dynamics, and whether lightweight retrieval-specific mechanisms could replace full attention lay-\n",
      "ers. Understanding this functional division advances both mechanistic interpretability and efficient\n",
      "architecture design, suggesting hybrid models operate as task-specific specialists rather than fully\n",
      "integrated systems.\n",
      "Acknowledgments and Disclosure of Funding\n",
      "This research was conducted as part of Felix Michalak‚Äôs Bachelor‚Äôs thesis at the University of\n",
      "Groningen under the supervision of Steven Abreu. Felix Michalak was responsible for the entirety of\n",
      "the code, experiments, result analysis, and discussion. Steven Abreu helped shape and streamline the\n",
      "project, as well as assisted with the writing process.\n",
      "References\n",
      "Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao,\n",
      "Ashish Rao, Atri Rudra, and Christopher R√©. Just Read Twice: closing the recall gap for recurrent\n",
      "language models, 2024. URL http://arxiv.org/abs/2407.05483. arXiv:2407.05483 [cs].\n",
      "Yushi Bai, Xin Lv, Jiajie Zhang, Yuze He, Ji Qi, Lei Hou, Jie Tang, Yuxiao Dong, and Juanzi Li.\n",
      "LongAlign: A Recipe for Long Context Alignment of Large Language Models, 2024. URL\n",
      "https://arxiv.org/abs/2401.18058.\n",
      "Aleksandar Botev, Soham De, Samuel L. Smith, Anushan Fernando, George-Cristian Muraru, Ruba\n",
      "Haroun, Leonard Berrada, Razvan Pascanu, Pier Giuseppe Sessa, Robert Dadashi, L√©onard\n",
      "Hussenot, Johan Ferret, Sertan Girgin, Olivier Bachem, Alek Andreev, Kathleen Kenealy, Thomas\n",
      "Mesnard, Cassidy Hardin, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi√®re,\n",
      "Mihir Sanjay Kale, Juliette Love, Pouya Tafti, Armand Joulin, Noah Fiedel, Evan Senter, Yutian\n",
      "Chen, Srivatsan Srinivasan, Guillaume Desjardins, David Budden, Arnaud Doucet, Sharad Vikram,\n",
      "Adam Paszke, Trevor Gale, Sebastian Borgeaud, Charlie Chen, Andy Brock, Antonia Paterson,\n",
      "Jenny Brennan, Meg Risdal, Raj Gundluru, Nesh Devanathan, Paul Mooney, Nilay Chauhan,\n",
      "Phil Culliton, Luiz Gustavo Martins, Elisa Bandy, David Huntsperger, Glenn Cameron, Arthur\n",
      "Zucker, Tris Warkentin, Ludovic Peran, Minh Giang, Zoubin Ghahramani, Cl√©ment Farabet, Koray\n",
      "Kavukcuoglu, Demis Hassabis, Raia Hadsell, Yee Whye Teh, and Nando de Frietas. Recurrent-\n",
      "Gemma: Moving Past Transformers for Efficient Open Language Models, August 2024. URL\n",
      "http://arxiv.org/abs/2404.07839. arXiv:2404.07839 [cs].\n",
      "Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster,\n",
      "Laurence Golding, Jeffrey Hsu, Alain Le Noac‚Äôh, Haonan Li, Kyle McDonell, Niklas Muennighoff,\n",
      "Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika,\n",
      "Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The Language Model Evaluation\n",
      "Harness, 07 2024. URL https://zenodo.org/records/12608602.\n",
      "Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob\n",
      "Steinhardt. Measuring Massive Multitask Language Understanding. CoRR, abs/2009.03300, 2020.\n",
      "URL https://arxiv.org/abs/2009.03300.\n",
      "Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi,\n",
      "Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida,\n",
      "Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam\n",
      "Rozen, Erez Shwartz, Mor Zusman, and Yoav Shoham. Jamba: A Hybrid Transformer-Mamba\n",
      "Language Model, July 2024. URL http://arxiv.org/abs/2403.19887. arXiv:2403.19887\n",
      "[cs].\n",
      "9\n",
      "Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava,\n",
      "Ce Zhang, Yuandong Tian, Christopher Re, and Beidi Chen. Deja Vu: Contextual Sparsity\n",
      "for Efficient LLMs at Inference Time. In Proceedings of the 40th International Conference on\n",
      "Machine Learning, pp. 22137‚Äì22176. PMLR, July 2023. URL https://proceedings.mlr.\n",
      "press/v202/liu23am.html. ISSN: 2640-3498.\n",
      "Catherine Olsson, Nelson Elhage, Neel Nanda, Nicholas Joseph, Nova DasSarma, Tom Henighan,\n",
      "Ben Mann, Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli,\n",
      "Zac Hatfield-Dodds, Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane\n",
      "Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish,\n",
      "and Chris Olah. In-context learning and induction heads. Transformer Circuits Thread, 2022.\n",
      "https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html.\n",
      "Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Al-\n",
      "bert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika\n",
      "Singh, Jared Casper, Jan Kautz, Mohammad Shoeybi, and Bryan Catanzaro. An Empirical Study\n",
      "of Mamba-based Language Models, June 2024. URL http://arxiv.org/abs/2406.07887.\n",
      "arXiv:2406.07887 [cs].\n",
      "Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\n",
      "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding,\n",
      "2019. URL https://arxiv.org/abs/1804.07461.\n",
      "Davide Zani, Felix Michalak, and Steven Abreu. Contextual Sparsity as a Tool for Mechanistic\n",
      "Understanding of Retrieval in Hybrid Foundation Models. In Sparsity in LLMs (SLLM): Deep\n",
      "Dive into Mixture of Experts, Quantization, Hardware, and Inference, 2025. URL https://\n",
      "openreview.net/forum?id=TGWzg86kYv.\n",
      "10\n",
      "A\n",
      "Model Specifications\n",
      "We used three different models: RecurrentGemma-2B (RG-2B), RecurrentGemma-9B (RG-9B), and\n",
      "Jamba-Mini-1.6 (Jamba).\n",
      "RG-2B was used by Zani et al. (2025) as a comparatively small hybrid LLM that is easy to run. We\n",
      "used this model to provide comparability to previous results. This model consists of 2.03B parameters\n",
      "(excluding embedding parameters) spread across 26 layers, being eight repetitions of the pattern \"2√ó\n",
      "SSM, 1√ó Attention\" followed by two SSM layers. Each attention layer hosts ten attention heads.\n",
      "RG-9B was used as a model of a larger size that is comparable to RG-2B. RG-9B and RG-2B are of\n",
      "the same structure and architecture, just that RG-9B is bigger in scale with 7.53B parameters spread\n",
      "across 38 layers, being 12 repetitions of the pattern \"2√ó SSM, 1√ó Attention\" followed by two SSM\n",
      "layers. The attention layers of RG-9B host 16 heads each.\n",
      "Both RG models use sliding window attention with a sliding window size of 2048.\n",
      "Jamba was used as a model of different architecture to compare to the RG models. Jamba uses global\n",
      "attention and Mixture of Experts (MoE) layers that systematically substitute Multilayer Perceptrons\n",
      "(MLPs). MoE layers are sparse alternatives to MLPs that only use a fraction of parameters during\n",
      "inference, effectively reducing the memory footprint of a model. Jamba consists of 51.03B total\n",
      "parameters (excluding embedding parameters), of which only 12B are active during inference, spread\n",
      "across 32 layers in a pattern of \"3√ó SSM, 1√ó Attention 4√ó SSM\". Each attention layer hosts 32\n",
      "attention heads.\n",
      "These three models built a diverse basis for experimental results, differing in their parameter sizes,\n",
      "attention implementation, and surrounding architecture.\n",
      "All experiments were conducted on a high-performance cluster, utilizing a maximum of two AMD\n",
      "Zen 3 EPYC 7763 processors, eight NVIDIA A100 (40 GB HBM2) graphics cards, and 1024 GB of\n",
      "memory.\n",
      "B\n",
      "NIAH Benchmark Implementation Details\n",
      "The Needle-In-A-Haystack (NIAH) benchmark is built on the implementation by Bai et al. (2024).\n",
      "The task in NIAH is to retrieve a specific sentence from a context that is filled with irrelevant text.\n",
      "The to-be-retrieved information is called the needle, and the whole context, including the irrelevant\n",
      "text and the needle, is called the haystack. The benchmark evaluates the ability of a model to retrieve\n",
      "the needle from the haystack for multiple prompts of different lengths. Through strategic prompt\n",
      "generation, this benchmark can yield retrieval maps that unveil the retrieval performance of a model,\n",
      "depending on the position of the needle and the size of the haystack. See Figure 4 for examples of\n",
      "such a retrieval map. Following Bai et al. (2024), we used the same collection of essays from Paul\n",
      "Graham3 to generate the haystack, as well as the same needle.\n",
      "The needle was \"The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a\n",
      "sunny day\". The retrieval question was \"What is the best thing to do in San Francisco?\".\n",
      "We chose a maximum prompt length of 4096 tokens. For each tokenizer, 100 prompts were generated:\n",
      "ten prompt lengths at ten depths.\n",
      "As an improvement over the scoring method used by Zani et al. (2025), which was a simple binary\n",
      "string matching evaluation, we implemented granular scoring. While still based on string matching,\n",
      "partial matches were allowed to yield partial scores. For each keyword of the needle that was present\n",
      "in the output string, the score was increased. The scores for all partial keywords added up to three.\n",
      "If the full needle string matched, the score was set to five. While examining outputs from test\n",
      "experiments on Jamba, we stumbled across a curious phenomenon: the grammatically imperfect\n",
      "needle was predicted with corrected grammar, thereby only scoring three points. To visualize this\n",
      "phenomenon, we added a rule: if the needle was predicted with corrected grammar, the score was set\n",
      "to four. See Table 2 for concrete strings and their scoring behavior.\n",
      "3All essays can be found in their original form at https://www.paulgraham.com/articles.html, and are also\n",
      "part of the GitHub repository for this project at https://github.com/lamalunderscore/retrieval-in-hybrid-ssms.\n",
      "11\n",
      "Keyword\n",
      "Score\n",
      "\"eat a sandwich\"\n",
      "increase by 1.0\n",
      "\"Dolores Park\"\n",
      "increase by 0.5\n",
      "\"sit in Dolores Park\"\n",
      "increase by 0.5\n",
      "\"sunny day\"\n",
      "increase by 1.0\n",
      "\"is to eat a sandwich and sit in Dolores Park on a sunny day\"\n",
      "set to 4.0\n",
      "\"The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park\n",
      "on a sunny day\"\n",
      "set to 5.0\n",
      "Table 2: Granular scoring system for NIAH evaluation. Keyword strings used in the evaluation of\n",
      "retrieval success, and their influence on the score. Notice the grammatical imperfection in the full\n",
      "needle string (last row).\n",
      "B.1\n",
      "Prompt templates used in NIAH\n",
      "For base models that were not instruction-tuned, we used the following template to style the prompts:\n",
      "CONTEXT:\n",
      "<haystack>\n",
      "QUESTION:\n",
      "<retrieval question>\n",
      "ANSWER: Here is the most relevant sentence in the context:\n",
      "For instruction-tuned models, we changed the template to:\n",
      "CONTEXT:\n",
      "<haystack>\n",
      "QUESTION:\n",
      "<retrieval question> Output the most relevant sentence in the context, word by word!\n",
      "C\n",
      "Comparing retrieval maps for JRT and standard NIAH\n",
      "In Section 3, we are also testing if applying JRT improves retrieval capabilities for top-k-sparsified\n",
      "RG-2B, RG-9B and Jamba. Figure 5, 6, and 7 compare the retrieval maps of standard NIAH and\n",
      "JRT-applied NIAH on the same k on all models. Refer to Figure 4 for an explanation of the axes.\n",
      "12\n",
      "(a) RG-2B - Base\n",
      "(b) RG-9B - Base\n",
      "(c) Jamba - Base\n",
      "(d) RG-2B - JRT\n",
      "(e) RG-9B - JRT\n",
      "(f) Jamba - JRT\n",
      "Figure 5: Retrieval maps for RG-2B, RG-9B and Jamba at k = 0 on NIAH with and without JRT\n",
      "applied.\n",
      "(a) RG-2B - Base\n",
      "(b) RG-9B - Base\n",
      "(c) Jamba - Base\n",
      "(d) RG-2B - JRT\n",
      "(e) RG-9B - JRT\n",
      "(f) Jamba - JRT\n",
      "Figure 6: Retrieval maps for RG-2B, RG-9B and Jamba at k = 1 on NIAH with and without JRT\n",
      "applied.\n",
      "13\n",
      "(a) RG-2B - Base\n",
      "(b) RG-9B - Base\n",
      "(c) Jamba - Base\n",
      "(d) RG-2B - JRT\n",
      "(e) RG-9B - JRT\n",
      "(f) Jamba - JRT\n",
      "Figure 7: Retrieval maps for RG-2B, RG-9B and Jamba at k = 2 on NIAH with and without JRT\n",
      "applied.\n",
      "D\n",
      "LME Benchmark\n",
      "In Section 4, we tested Jamba using LME. It is an instruction-tuned model, so evaluation was run\n",
      "with the apply_chat_template and fewshot_as_multiturn flags. Figures 8 and 9 show the\n",
      "performance of Jamba in the ablated, sparsified and base version per subtask on GLUE and MMLU.\n",
      "MNLI\n",
      "MNLI-MM\n",
      "MRPC\n",
      "QNLI\n",
      "QQP\n",
      "RTE\n",
      "SST2\n",
      "WNLI\n",
      "CoLA\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "Ablated\n",
      "Sparse\n",
      "Base\n",
      "GLUE Scores\n",
      "Ablated/Sparse/Base Jamba-Mini-1.6 by Sub Task\n",
      "Sub Task\n",
      "Score in %\n",
      "Random Guessing Baseline\n",
      "Figure 8: GLUE benchmark Sub Task scores of the ablated (k = 0), sparse (k = 5) and base\n",
      "configuration.\n",
      "E\n",
      "Retrieval maps for attention weight manipulation\n",
      "In Section 5, we systematically manipulated the attention weights in RG-2B during NIAH benchmarks.\n",
      "Figure 10 shows the retrieval maps of those benchmarks, structured in the same way as Table 1 for\n",
      "easy comparison.\n",
      "14\n",
      "Humanities\n",
      "High School European History\n",
      "High School World History\n",
      "Jurisprudence\n",
      "Moral Disputes\n",
      "Philosophy\n",
      "Professional Law\n",
      "Other\n",
      "Clinical Knowledge\n",
      "Global Facts\n",
      "Management\n",
      "Medical Genetics\n",
      "Nutrition\n",
      "Professional Medicine\n",
      "Social Sciences\n",
      "High School Geography\n",
      "High School Macroeconomics\n",
      "High School Psychology\n",
      "Professional Psychology\n",
      "Security Studies\n",
      "Us Foreign Policy\n",
      "Abstract Algebra\n",
      "Astronomy\n",
      "College Chemistry\n",
      "College Mathematics\n",
      "Computer Security\n",
      "Electrical Engineering\n",
      "High School Biology\n",
      "High School Computer Science\n",
      "High School Physics\n",
      "Machine Learning\n",
      "0\n",
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "Ablated\n",
      "Sparse\n",
      "Base\n",
      "MMLU Scores, Ablated/Sparse/Base Jamba-Mini-1.6 by Sub Task\n",
      "Sub Task\n",
      "Score in %\n",
      "Random Guessing Baseline\n",
      "Figure 9: MMLU benchmark Sub Task scores of the ablated (k = 0), sparse (k = 5) and base\n",
      "configuration.\n",
      "0.0\n",
      "11.0\n",
      "22.0\n",
      "33.0\n",
      "44.0\n",
      "56.0\n",
      "67.0\n",
      "78.0\n",
      "89.0\n",
      "100.0\n",
      "Depth Percent\n",
      "Keep-Keep\n",
      "Omit-Keep\n",
      "Only-Keep\n",
      "Balanced-Keep\n",
      "0.0\n",
      "11.0\n",
      "22.0\n",
      "33.0\n",
      "44.0\n",
      "56.0\n",
      "67.0\n",
      "78.0\n",
      "89.0\n",
      "100.0\n",
      "Depth Percent\n",
      "Keep-Omit\n",
      "Omit-Omit\n",
      "Only-Omit\n",
      "Balanced-Omit\n",
      "0.0\n",
      "11.0\n",
      "22.0\n",
      "33.0\n",
      "44.0\n",
      "56.0\n",
      "67.0\n",
      "78.0\n",
      "89.0\n",
      "100.0\n",
      "Depth Percent\n",
      "Keep-Only\n",
      "Omit-Only\n",
      "Only-Only\n",
      "Balanced-Only\n",
      "0.0\n",
      "11.0\n",
      "22.0\n",
      "33.0\n",
      "44.0\n",
      "56.0\n",
      "67.0\n",
      "78.0\n",
      "89.0\n",
      "100.0\n",
      "Depth Percent\n",
      "Keep-Balanced\n",
      "Omit-Balanced\n",
      "Only-Balanced\n",
      "Balanced-Balanced\n",
      "250\n",
      "677\n",
      "1105\n",
      "1532\n",
      "1959\n",
      "2387\n",
      "2814\n",
      "3241\n",
      "3669\n",
      "4096\n",
      "Token Limit\n",
      "0.0\n",
      "11.0\n",
      "22.0\n",
      "33.0\n",
      "44.0\n",
      "56.0\n",
      "67.0\n",
      "78.0\n",
      "89.0\n",
      "100.0\n",
      "Depth Percent\n",
      "Keep-Null\n",
      "250\n",
      "677\n",
      "1105\n",
      "1532\n",
      "1959\n",
      "2387\n",
      "2814\n",
      "3241\n",
      "3669\n",
      "4096\n",
      "Token Limit\n",
      "Omit-Null\n",
      "250\n",
      "677\n",
      "1105\n",
      "1532\n",
      "1959\n",
      "2387\n",
      "2814\n",
      "3241\n",
      "3669\n",
      "4096\n",
      "Token Limit\n",
      "Only-Null\n",
      "250\n",
      "677\n",
      "1105\n",
      "1532\n",
      "1959\n",
      "2387\n",
      "2814\n",
      "3241\n",
      "3669\n",
      "4096\n",
      "Token Limit\n",
      "Balanced-Null\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Score (0 = Failure, 5 = Perfect)\n",
      "Figure 10: Retrieval maps for all manipulation combinations used in Section 5.\n",
      "15\n",
      "F\n",
      "Pearls of Wisdom\n",
      "Investigating and experimenting with manipulated models often results in amusing outputs. Since\n",
      "life is about joy, and reading some of those outputs definitely caused us joy, we distilled a collection\n",
      "of output strings to share our laughter with the world. Some quotes are also absurdly wise, which led\n",
      "us to believe they were part of the Paul Graham essays that were used to fill the context in the NIAH\n",
      "benchmark. However, none of the generated strings were found in the context strings. All following\n",
      "quotes were generated by sparsified or manipulated RG-2B or RG-9B, in response to the question:\n",
      "\"What is the best thing to do in San Francisco?\"\n",
      "The best way to get to the top of the mountain is to start by walking.\n",
      "- Mark Twain\n",
      "The first thing I want to say is that I am not a native English speaker\n",
      "RG-2B - Binary-Keep\n",
      "\"The most important thing is to be kind. It is the little things that make the difference.\"\n",
      "- Unknown\n",
      "RG-2B - Binary-Keep\n",
      "The first sentence is a sentence that is not a sentence. It is a sentence that has been written in a way that is not a\n",
      "sentence.\n",
      "RG-2B - Binary-Null\n",
      "\"The best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day on a sunny day.\"\n",
      "The above is a joke.\n",
      "RG-2B - Only-Keep\n",
      "I am a programmer. I am a programmer. I am a programmer. I am a programmer. I am a programmer. I am a\n",
      "programmer. I am a programmer.\n",
      "RG-2B - k = 0, sparse prefill\n",
      "You need humility to know when to use qualification.\n",
      "RG-9B - k = 4\n",
      "16\n",
      "\n",
      "üìò Paper 3:\n",
      "{'Published': '2021-07-16', 'Title': 'All the attention you need: Global-local, spatial-channel attention for image retrieval', 'Authors': 'Chull Hwan Song, Hye Joo Han, Yannis Avrithis', 'Summary': 'We address representation learning for large-scale instance-level image\\nretrieval. Apart from backbone, training pipelines and loss functions, popular\\napproaches have focused on different spatial pooling and attention mechanisms,\\nwhich are at the core of learning a powerful global image representation. There\\nare different forms of attention according to the interaction of elements of\\nthe feature tensor (local and global) and the dimensions where it is applied\\n(spatial and channel). Unfortunately, each study addresses only one or two\\nforms of attention and applies it to different problems like classification,\\ndetection or retrieval.\\n  We present global-local attention module (GLAM), which is attached at the end\\nof a backbone network and incorporates all four forms of attention: local and\\nglobal, spatial and channel. We obtain a new feature tensor and, by spatial\\npooling, we learn a powerful embedding for image retrieval. Focusing on global\\ndescriptors, we provide empirical evidence of the interaction of all forms of\\nattention and improve the state of the art on standard benchmarks.'}\n",
      "\n",
      "üìò Title:\n",
      "All the attention you need: Global-local, spatial-channel attention for image retrieval\n",
      "\n",
      "üìò Published::\n",
      "2021-07-16\n",
      "\n",
      "üìò Authors::\n",
      "Chull Hwan Song, Hye Joo Han, Yannis Avrithis\n",
      "\n",
      "All the attention you need:\n",
      "Global-local, spatial-channel attention for image retrieval\n",
      "Chull Hwan Song\n",
      "Odd Concepts\n",
      "Hye Joo Han\n",
      "Odd Concepts\n",
      "Yannis Avrithis\n",
      "Inria, Univ Rennes, CNRS, IRISA\n",
      "Abstract\n",
      "We address representation learning for large-scale\n",
      "instance-level image retrieval. Apart from backbone, train-\n",
      "ing pipelines and loss functions, popular approaches have\n",
      "focused on different spatial pooling and attention mecha-\n",
      "nisms, which are at the core of learning a powerful global\n",
      "image representation. There are different forms of attention\n",
      "according to the interaction of elements of the feature tensor\n",
      "(local and global) and the dimensions where it is applied\n",
      "(spatial and channel). Unfortunately, each study addresses\n",
      "only one or two forms of attention and applies it to different\n",
      "problems like classiÔ¨Åcation, detection or retrieval.\n",
      "We present global-local attention module (GLAM),\n",
      "which is attached at the end of a backbone network and\n",
      "incorporates all four forms of attention: local and global,\n",
      "spatial and channel. We obtain a new feature tensor and, by\n",
      "spatial pooling, we learn a powerful embedding for image\n",
      "retrieval. Focusing on global descriptors, we provide em-\n",
      "pirical evidence of the interaction of all forms of attention\n",
      "and improve the state of the art on standard benchmarks.\n",
      "1. Introduction\n",
      "Instance-level image retrieval is at the core of visual rep-\n",
      "resentation learning and is connected with many problems\n",
      "of visual recognition and machine learning, for instance\n",
      "metric learning [30, 26], few-shot learning [42] and unsu-\n",
      "pervised learning [8]. Many large-scale open datasets [3,\n",
      "37, 16, 29, 53], and competitions1 have accelerated progress\n",
      "in instance-level image retrieval, which has been trans-\n",
      "formed by deep learning [3].\n",
      "Many studies on instance-level image retrieval focus\n",
      "on learning features from convolutional neural networks\n",
      "(CNN), while others focus on re-ranking, for instance by\n",
      "graph-based methods [11]. The former can be distinguished\n",
      "according to feature types: local descriptors, reminiscent of\n",
      "SIFT [27], where an image is mapped to a few hundred vec-\n",
      "tors; and global descriptors, where an image is mapped to a\n",
      "1https://www.kaggle.com/c/landmark-retrieval-2020\n",
      "single vector. In fact, deep learning has brought global de-\n",
      "scriptors with astounding performance, while allowing efÔ¨Å-\n",
      "cient search. Our study belongs to this type.\n",
      "Studies on global descriptors have focused on spatial\n",
      "pooling [2, 37]. The need for compact, discriminative rep-\n",
      "resentations that are resistant to clutter has naturally given\n",
      "rise to spatial attention methods [24, 28]. Different kinds\n",
      "of attention have been studied in many areas of computer\n",
      "vision research. There is also channel attention [20, 9]; lo-\n",
      "cal attention, applied independently to elements of the rep-\n",
      "resentation (feature map) [54, 25]; global attention, based\n",
      "on interaction between elements [52, 9]; and combinations\n",
      "thereof. Unfortunately, each study has been limited to one or\n",
      "two kinds of attention only; attention is not always learned;\n",
      "and applications vary.\n",
      "It is the objective of our work to perform a compre-\n",
      "hensive study of all forms of attention above, apply them\n",
      "to instance-level image retrieval and provide a detailed ac-\n",
      "count of their interaction and impact on performance. As\n",
      "shown in Figure 1, we collect contextual information from\n",
      "images with both local and global attention, giving rise to\n",
      "two parallel network streams. Importantly, each operates\n",
      "on both spatial locations and feature channels. Local at-\n",
      "tention is about individual locations and channels; global is\n",
      "about interaction between locations and between channels.\n",
      "The extracted information is separately embedded in local\n",
      "and global attention feature maps, which are combined in a\n",
      "global-local attention feature map before pooling.\n",
      "Our contributions can be summarized as follows:\n",
      "1. We propose a novel network that consists of both\n",
      "global and local attention for image retrieval. This is\n",
      "the Ô¨Årst study that employs both mechanisms.\n",
      "2. Each of the global and local attention mechanisms\n",
      "comprises both spatial and channel attention.\n",
      "3. Focusing on global descriptors, we provide empirical\n",
      "evidence of the interaction of all forms of attention and\n",
      "improve the state of the art on standard benchmarks.\n",
      "1\n",
      "arXiv:2107.08000v1  [cs.CV]  16 Jul 2021\n",
      "Al\n",
      "c\n",
      "c √ó 1 √ó 1\n",
      "√ó\n",
      "+\n",
      "Fl\n",
      "c\n",
      "Al\n",
      "s\n",
      "1 √ó h √ó w\n",
      "√ó\n",
      "+\n",
      "Fl\n",
      "√ó\n",
      "c √ó h √ó w\n",
      "F\n",
      "√ó\n",
      "+\n",
      "c √ó h √ó w\n",
      "Fgl\n",
      "Ag\n",
      "c\n",
      "c √ó c\n",
      "√ó\n",
      "Fg\n",
      "c\n",
      "Ag\n",
      "s\n",
      "hw √ó hw\n",
      "√ó\n",
      "+\n",
      "Fg\n",
      "√ó\n",
      "wl\n",
      "w\n",
      "wg\n",
      "channel attention\n",
      "spatial attention\n",
      "fusion\n",
      "local attention\n",
      "global attention\n",
      "Figure 1: Our global-local attention module (GLAM) involves both channel and spatial attention, as well as both local atten-\n",
      "tion (channels/locations weighted independently, based on contextual information obtained by pooling) and global attention\n",
      "(based on pairwise interaction between channels/locations). As a result, four attention maps are used: local channel (Al\n",
      "c),\n",
      "local spatial (Al\n",
      "s), global channel (Ag\n",
      "c) and global spatial (Ag\n",
      "s). The input feature map F is weighted into local (Fl) and\n",
      "global (Fg) attention feature maps, which are fused with F to yield the global-local attention feature map Fgl. The diagram\n",
      "is abstract: The four attention modules are shown in more detail in Figures 2, 3, 4, 5.\n",
      "2. Related work\n",
      "Instance-level image retrieval\n",
      "Studies on instance-level\n",
      "image retrieval can be roughly, but not exclusively, di-\n",
      "vided into three types: (1) studies on global descriptors\n",
      "[3, 16, 24, 53, 2, 37]; (2) studies on local descriptors and\n",
      "geometry-based re-ranking [29, 45, 40, 53]; (3) re-ranking\n",
      "by graph-based methods [11, 21, 55]. The Ô¨Årst two types\n",
      "of studies focus on the feature representation, while the last\n",
      "type focuses on re-ranking extracted features.\n",
      "Studies on global descriptors focus on spatial pooling\n",
      "of CNN feature maps into vectors, including MAC [38],\n",
      "SPoC [2], CroW [24], R-MAC [48, 15, 16], GeM [37],\n",
      "and NetVLAD [1, 25], as well as learning the representa-\n",
      "tion [3, 15, 16, 36, 37]. Studies before deep learning dom-\n",
      "inated image retrieval were mostly based on local descrip-\n",
      "tors like SIFT [27] and bag-of-words representation [32] or\n",
      "aggregated descriptors like VLAD [22] or ASMK [46]. Lo-\n",
      "cal descriptors have been revived in deep learning, e.g. with\n",
      "DELF [29], DELG [5] and ASMK extensions [45, 47].\n",
      "We focus on learning a global descriptor in this work, be-\n",
      "cause it is the most efÔ¨Åcient in terms of storage and search.\n",
      "However, our generic attention mechanism produces a fea-\n",
      "ture tensor and could be applicable to local descriptors as\n",
      "well, if global pooling were replaced by local feature detec-\n",
      "tion. Re-ranking methods are complementary to the repre-\n",
      "sentation and we do not consider them in this work.\n",
      "Attention\n",
      "Attention mechanisms have been Ô¨Årst proposed\n",
      "in image classiÔ¨Åcation studies focusing on channel at-\n",
      "METHOD\n",
      "LOCAL\n",
      "GLOBAL\n",
      "LRN RET\n",
      "Spatial Channel Spatial Channel\n",
      "SENet [20]\n",
      "‚úì\n",
      "‚úì\n",
      "ECA-Net [51]\n",
      "‚úì\n",
      "‚úì\n",
      "GCNet [6]\n",
      "‚úì\n",
      "‚úì\n",
      "CBAM [54]\n",
      "‚úì\n",
      "‚úì\n",
      "‚úì\n",
      "GE [19]\n",
      "‚úì\n",
      "‚úì\n",
      "NL-Net [52]\n",
      "‚úì\n",
      "‚úì\n",
      "AA-Net [4]\n",
      "‚úì\n",
      "‚úì\n",
      "SAN [59]\n",
      "‚úì\n",
      "‚úì\n",
      "N3Net [34]\n",
      "‚úì\n",
      "‚úì\n",
      "A2-Net [9]\n",
      "‚úì\n",
      "‚úì\n",
      "GSoP [14]\n",
      "‚úì\n",
      "‚úì\n",
      "OnA [23]\n",
      "‚úì\n",
      "‚úì\n",
      "AGeM [17]\n",
      "‚úì\n",
      "‚úì\n",
      "CroW [24]\n",
      "‚úì\n",
      "‚úì\n",
      "‚úì\n",
      "CRN [25]\n",
      "‚úì\n",
      "‚úì\n",
      "‚úì\n",
      "DELF [29]\n",
      "‚úì\n",
      "‚úì\n",
      "‚úì\n",
      "DELG [5]\n",
      "‚úì\n",
      "‚úì\n",
      "‚úì\n",
      "Tolias et al. [47]\n",
      "‚úì\n",
      "‚úì\n",
      "‚úì\n",
      "SOLAR [28]\n",
      "‚úì\n",
      "‚úì\n",
      "‚úì\n",
      "Ours\n",
      "‚úì\n",
      "‚úì\n",
      "‚úì\n",
      "‚úì\n",
      "‚úì\n",
      "‚úì\n",
      "Table 1: Related work on attention. LRN: learned; RET: ap-\n",
      "plied to instance-level image retrieval.\n",
      "tention [20, 51, 6], spatial attention [19] or both, like\n",
      "CBAM [54]. In image retrieval, CroW [24] also employs\n",
      "2\n",
      "feature map\n",
      "GAP\n",
      "conv1d(k)\n",
      "sigmoid\n",
      "attention map\n",
      "c √ó h √ó w\n",
      "c √ó 1 √ó 1\n",
      "c √ó 1 √ó 1\n",
      "F\n",
      "Al\n",
      "c\n",
      "Figure 2: Local channel attention.\n",
      "both spatial and channel attention and can be seen as a pre-\n",
      "cursor of CBAM, but, like other studies of spatial attention\n",
      "on retrieval [41, 23, 17], it is not learned. CRN [25] ap-\n",
      "plies spatial attention for feature reweighting and is learned.\n",
      "Learned spatial attention mechanisms are common for local\n",
      "descriptors [29, 5, 47].\n",
      "We call the above methods local attention, in the sense\n",
      "that elements of the feature tensor (channels / spatial loca-\n",
      "tions), are weighted independently, based on contextual in-\n",
      "formation obtained by pooling or learned. By constrast, by\n",
      "global attention we refer to mechanisms that model inter-\n",
      "action between elements of the feature tensor, for example\n",
      "between channels or between locations.\n",
      "In image classiÔ¨Åcation, non-local neural network (NL-\n",
      "Net) [52] is maybe the Ô¨Årst global attention mechanism, fol-\n",
      "lowed by similar studies [4, 59, 34]. It is global spatial at-\n",
      "tention, allowing interaction between any pair of spatial lo-\n",
      "cations. Similarly, there are studies of global channel atten-\n",
      "tion, allowing interaction between channels [9, 14]. Global\n",
      "attention has focused mostly on image recognition and has\n",
      "been applied to either spatial or channel attention so far, not\n",
      "both. In image retrieval, SOLAR [28] is a direct application\n",
      "of the global spatial attention mechanism of [52].\n",
      "Table 1 attempts to categorize related work on atten-\n",
      "tion according to whether attention is local or global, spa-\n",
      "tial or channel, whether it is learned and whether it is ap-\n",
      "plied to instance-level image retrieval. We observe that all\n",
      "methods limit to one or two forms of attention only. Of\n",
      "those studies that focus on image retrieval, many are not\n",
      "learned [23, 17, 24], and of those that are, some are de-\n",
      "signed for local descriptors [29, 47].\n",
      "By contrast, we provide a comprehensive study of all\n",
      "forms of attention, global and local, spatial and channel, to\n",
      "obtain a learned representation in the form of a tensor that\n",
      "can be used in any way. We spatially pool it into a global\n",
      "descriptor and we study the relative gain of different forms\n",
      "of attention in image retrieval.\n",
      "feature map\n",
      "conv 1 √ó 1\n",
      "conv 3 √ó 3\n",
      "conv 5 √ó 5\n",
      "conv 7 √ó 7\n",
      "concat\n",
      "conv 1 √ó 1\n",
      "attention map\n",
      "c √ó h √ó w\n",
      "4c‚Ä≤ √ó h √ó w\n",
      "1 √ó h √ó w\n",
      "c‚Ä≤ √ó h √ó w\n",
      "dilated\n",
      "conv\n",
      "F\n",
      "F‚Ä≤\n",
      "Al\n",
      "s\n",
      "Figure 3: Local spatial attention. Convolutional layers in\n",
      "blue implemented by dilated convolutions with kernel size\n",
      "3 √ó 3 and dilation factors 1, 3, 5.\n",
      "3. Global-local attention\n",
      "We design a global-local attention module (GLAM),\n",
      "which is attached at the end of a backbone network. Figure 1\n",
      "illustrates its main components. We are given a c √ó h √ó w\n",
      "feature tensor F, where c is the number of channels, and\n",
      "h √ó w is the spatial resolution. Local attention collects con-\n",
      "text from the image and applies pooling to obtain a c√ó1√ó1\n",
      "local channel attention map Al\n",
      "c and a 1 √ó h √ó w local spa-\n",
      "tial attention map Al\n",
      "s. Global attention allows interaction\n",
      "between channels, resulting in a c √ó c global channel at-\n",
      "tention map Ag\n",
      "c, and between spatial locations, resulting in\n",
      "a hw √ó hw global spatial attention map Ag\n",
      "s. The feature\n",
      "maps produced by the two attention streams are combined\n",
      "with the original one by a learned fusion mechanism into\n",
      "the global-local attention feature map Fgl before being spa-\n",
      "tially pooled into a global image descriptor.\n",
      "3.1. Local attention\n",
      "We extract an 1D channel and a 2D spatial attention map\n",
      "to weigh the feature map in the corresponding dimensions.\n",
      "Local channel attention\n",
      "Following ECA-Net [51], this\n",
      "attention captures local channel information. As shown in\n",
      "Figure 2, we are given a c√óh√ów feature tensor F from our\n",
      "backbone. We Ô¨Årst reduce it to a c √ó 1 √ó 1 tensor by global\n",
      "average pooling (GAP). Channel attention is then captured\n",
      "by a 1D convolution of kernel size k along the channel di-\n",
      "mension, where k controls the extent of cross-channel inter-\n",
      "action. This is followed by a sigmoid function, resulting in\n",
      "the c √ó 1 √ó 1 local channel attention map Al\n",
      "c.\n",
      "Local spatial attention\n",
      "Inspired by the inception mod-\n",
      "ule [43] and similar to [25], this attention map captures local\n",
      "spatial information at different scales. As shown in Figure 3,\n",
      "3\n",
      "feature map\n",
      "GAP\n",
      "conv1d(k)\n",
      "conv1d(k)\n",
      "sigmoid\n",
      "sigmoid\n",
      "√ó\n",
      "√ó\n",
      "softmax\n",
      "attention feature map\n",
      "1 √ó c\n",
      "1 √ó c\n",
      "1 √ó c\n",
      "Qc\n",
      "c √ó c\n",
      "hw √ó c\n",
      "Vc\n",
      "Ag\n",
      "c\n",
      "c √ó h √ó w\n",
      "1 √ó c\n",
      "1 √ó c\n",
      "Kc\n",
      "F\n",
      "Gc\n",
      "Figure 4: Global channel attention.\n",
      "given the same c √ó h √ó w feature tensor F from our back-\n",
      "bone, we obtain a new tensor F‚Ä≤ with channels reduced to\n",
      "c‚Ä≤, using a 1 √ó 1 convolution. We then extract local spatial\n",
      "contextual information using convolutional Ô¨Ålters of kernel\n",
      "size 3 √ó 3, 5 √ó 5, and 7 √ó 7, which are efÔ¨Åciently imple-\n",
      "mented by 3 √ó 3 dilated convolutions [7, 57] with dilation\n",
      "parameter 1, 2, and 3 respectively. The resulting features,\n",
      "along with one obtained by 1 √ó 1 convolution on F‚Ä≤, are\n",
      "concatenated into a 4c‚Ä≤ √ó h √ó w tensor. Finally, we obtain\n",
      "the 1 √ó h √ó w local spatial attention map Al\n",
      "s by a 1 √ó 1\n",
      "convolution that reduces the channel dimension to 1.\n",
      "The middle column of Figure 6 shows heat maps of local\n",
      "spatial attention, localizing target objects in images.\n",
      "Local attention feature map\n",
      "We use the local channel\n",
      "attention map Al\n",
      "c to weigh F in the channel dimension\n",
      "Fl\n",
      "c := F ‚äôAl\n",
      "c + F.\n",
      "(1)\n",
      "We then use local spatial attention map Al\n",
      "s to weigh Fl\n",
      "c\n",
      "in the spatial dimensions, resulting in the c √ó h √ó w local\n",
      "attention feature map\n",
      "Fl = Fl\n",
      "c ‚äôAl\n",
      "s + Fl\n",
      "c.\n",
      "(2)\n",
      "Here, A‚äôB denotes an element-wise multiplication of ten-\n",
      "sors A and B, with broadcasting when one tensor is smaller.\n",
      "We adopt the choice of applying channel followed by spa-\n",
      "tial attention from convolutional block attention module\n",
      "CBAM [54]. However, apart from computing Al\n",
      "s at differ-\n",
      "ent scales, both attention maps are obtained from the orig-\n",
      "inal tensor F rather than sequentially. In addition, both (1)\n",
      "and (2) include residual connections, while CBAM includes\n",
      "a single residual connection over both steps.\n",
      "3.2. Global attention\n",
      "We extract two matrices capturing global pairwise chan-\n",
      "nel and spatial interaction to weigh the feature map.\n",
      "feature map\n",
      "conv 1 √ó 1\n",
      "conv 1 √ó 1\n",
      "conv 1 √ó 1\n",
      "√ó\n",
      "√ó\n",
      "softmax\n",
      "conv 1 √ó 1\n",
      "attention feature map\n",
      "c‚Ä≤ √ó hw\n",
      "Qs\n",
      "hw √ó hw\n",
      "c √ó h √ó w\n",
      "c‚Ä≤ √ó hw\n",
      "Vs\n",
      "c‚Ä≤ √ó h √ó w\n",
      "Ag\n",
      "s\n",
      "c √ó h √ó w\n",
      "c‚Ä≤ √ó hw\n",
      "Kc\n",
      "F\n",
      "Gs\n",
      "Figure 5: Global spatial attention.\n",
      "Global channel attention\n",
      "We introduce a global channel\n",
      "attention mechanism that captures global channel interac-\n",
      "tion. This mechanism is based on the non-local neural net-\n",
      "work [52], but with the idea of 1D convolution from ECA-\n",
      "Net [51]. As shown in Figure 4, we are given the c √ó h √ó w\n",
      "feature tensor F from our backbone. We apply GAP and\n",
      "squeeze spatial dimensions, followed by a 1D convolution\n",
      "of kernel size k and a sigmoid function, to obtain 1√óc query\n",
      "Qc and key Kc tensors. The value tensor Vc is obtained by\n",
      "mere reshaping of F to hw√óc, without GAP. Next, we form\n",
      "the outer product of Kc and Qc, followed by softmax over\n",
      "channels to obtain a c √ó c global channel attention map\n",
      "Ag\n",
      "c = softmax(Kc\n",
      "‚ä§Qc).\n",
      "(3)\n",
      "Finally, this attention map is multiplied with Vc and the ma-\n",
      "trix product VcAg\n",
      "c is reshaped back to c√óh√ów to give the\n",
      "global channel attention feature map Gc. In GSoP [14] and\n",
      "A2-Net [9], a c√óc global channel attention map is obtained\n",
      "by multiplication of hw √ó c matrices; (3) is more efÔ¨Åcient,\n",
      "using only an outer product of 1 √ó c vectors.\n",
      "Global spatial attention\n",
      "Since ordinary convolution ap-\n",
      "plies only a local neighborhood at a time, it cannot capture\n",
      "global contextual information. Thus, we apply non-local Ô¨Ål-\n",
      "tering [52], which is a form of self-attention [49] in the spa-\n",
      "tial dimensions. As shown in Figure 5, we are given the\n",
      "same c √ó h √ó w feature tensor F from our backbone. By\n",
      "using three 1√ó1 convolutions, which reduce channels to c‚Ä≤,\n",
      "and Ô¨Çattening spatial dimensions to hw, we obtain c‚Ä≤ √ó hw\n",
      "query Qs, key Ks, and value Vs tensors, where each col-\n",
      "umn is a feature vector corresponding to a particular spatial\n",
      "location. We capture pairwise similarities of these vectors\n",
      "by matrix multiplication of Ks and Qs, followed by soft-\n",
      "max over locations to obtain a hw √ó hw global spatial at-\n",
      "tention map:\n",
      "Ag\n",
      "s = softmax(K‚ä§\n",
      "s Qs).\n",
      "(4)\n",
      "4\n",
      "This attention map is multiplied with Vs and the matrix\n",
      "product VsAg\n",
      "s is reshaped back to c‚Ä≤ √óh√ów by expanding\n",
      "the spatial dimensions. Finally, using a 1 √ó 1 convolution,\n",
      "which increases channels back to c, we obtain the c√óh√ów\n",
      "global spatial attention feature map Gs.\n",
      "The right column of Figure 6 shows heat maps for global\n",
      "spatial attention, localizing target objects in images.\n",
      "Global attention feature map\n",
      "We use the global channel\n",
      "attention feature map Fc to weigh F element-wise\n",
      "Fg\n",
      "c = F ‚äôGc.\n",
      "(5)\n",
      "We then use global spatial attention feature map Gs to\n",
      "weigh Fg\n",
      "c element-wise, resulting in the c √ó h √ó w global\n",
      "attention feature map\n",
      "Fg = Fg\n",
      "c ‚äôGs + Fg\n",
      "c.\n",
      "(6)\n",
      "Similarly to Fl in (1) and (2), we apply channel attention\n",
      "Ô¨Årst, followed by spatial attention. However, unlike (1),\n",
      "there is no residual connection in (5). This choice is sup-\n",
      "ported by early experiments.\n",
      "3.3. Global-local attention\n",
      "Feature fusion\n",
      "As shown in Figure 1, we combine the\n",
      "local and global attention feature maps, Fl and Fg, with\n",
      "the original feature F. While concatenation and summation\n",
      "are common operations for feature combination, we use a\n",
      "weighted average with weights wl, wg, w respectively, ob-\n",
      "tained by softmax over three learnable scalar parameters, to\n",
      "obtain a c √ó h √ó w global-local attention feature map\n",
      "Fgl = wlFl + wgFl + wF.\n",
      "(7)\n",
      "EfÔ¨ÅcientDet [44] has shown that this is the most effective,\n",
      "among a number of choices, for fusion of features across\n",
      "different scales.\n",
      "Pooling\n",
      "We apply GeM [37], a learnable spatial pooling\n",
      "mechanism, to feature map Fgl (7), followed by a fully-\n",
      "connected (FC) layer with dropout and batch normalization.\n",
      "The Ô¨Ånal embedding is obtained by ‚Ñì2-normalization.\n",
      "4. Experiments\n",
      "4.1. Datasets\n",
      "Training set\n",
      "There are a number of open landmark\n",
      "datasets commonly used for training in image retrieval stud-\n",
      "ies, including neural code (NC) [3], neural code clean (NC-\n",
      "clean) [16], as well as Google Landmarks v1 (GLDv1) [29]\n",
      "and v2 (GLDv2) [53]. Table 2 shows relevant statistics.\n",
      "These datasets can be categorized into noisy and clean. The\n",
      "clean sets were obtained from the original noisy sets for\n",
      "more effective training [16, 53]. The original noisy datasets\n",
      "are much larger, but they have high intra-class variability.\n",
      "(a) input\n",
      "(b) local\n",
      "(c) global\n",
      "Figure 6: Local and global spatial attention. Left: input\n",
      "images. Middle: local spatial attention heat maps. Right:\n",
      "global spatial attention heat maps. Red (blue) means higher\n",
      "(lower) attention weight.\n",
      "Each class can include visually dissimilar images such as\n",
      "exterior and interior views of a building or landmark, in-\n",
      "cluding Ô¨Çoor plans and paintings inside. The clean datasets\n",
      "focus on views directly relevant to landmark recognition but\n",
      "have a much smaller number of images.\n",
      "Evaluation set and metrics\n",
      "We use four common eval-\n",
      "uation datasets for landmark image retrieval: Oxford5k\n",
      "(Ox5k) [32], Paris6k (Par6k) [33], as well as Revisited Ox-\n",
      "ford (ROxford or ROxf) and Paris (RParis or RPar) [35].\n",
      "ROxford and RParis are used with and without one million\n",
      "distractors (R1M) [28] and evaluated using the Medium and\n",
      "Hard protocols [35]. We evaluate using mean Average Pre-\n",
      "cision (mAP) and mean precision at 10 (mP@10).\n",
      "4.2. Implementation details\n",
      "We train on 8 TITAN RTX 2080Ti GPUs. All models are\n",
      "pre-trained on ImageNet [39] and implemented in PyTorch\n",
      "[31]. For fair comparisons, we set a training environment\n",
      "5\n",
      "Figure 7: Examples of our ranking results. In each row, the Ô¨Årst image on the left (pink dotted outline) is a query image with a\n",
      "target object (red crop box), and the following are the top ranking images for the query. Orange solid outline: positive images\n",
      "for the query; red solid outline: negative.\n",
      "similar to the those of compared studies [56, 53, 28, 35]. We\n",
      "employ ResNet101 [18] as a backbone model. The kernel\n",
      "size k of ECANet in subsection 3.1 is set to 3. The param-\n",
      "eter p of GeM in subsection 3.3 is set to 3 and the dimen-\n",
      "sion d of Ô¨Ånal embeddings to 512. We adopt ArcFace [10],\n",
      "a cosine-softmax based loss, with a margin of 0.3. We use\n",
      "stochastic gradient descent with initial learning rate 10‚àí3,\n",
      "momentum 0.9 and weight decay 10‚àí5.\n",
      "We adopt the batch sampling of Yokoo et al. [56] where\n",
      "mini-batch samples with similar aspect ratios are resized to\n",
      "a particular size. Here, we use a batch size of 64. For image\n",
      "augmentation, we apply scaling, random cropping, and var-\n",
      "ied illumination. At inference, we apply a multi-resolution\n",
      "representation [16] to query and database images.\n",
      "Our method is denoted as GLAM (global-local atten-\n",
      "tion module). Using the backbone model alone is referred\n",
      "to as baseline. It is compatible with recent models based\n",
      "on ResNet101-GeM trained with ArcFace [53, 28]. Adding\n",
      "our local attention (subsection 3.1) to the baseline model is\n",
      "denoted +local, while adding our global attention (subsec-\n",
      "tion 3.2) is denoted +global. Since we focus on representa-\n",
      "tion learning, we do not consider post-processing methods\n",
      "like geometry-based re-ranking [29, 40, 53] or graph-based\n",
      "re-ranking [11, 21, 55].\n",
      "4.3. Benchmarking\n",
      "Noisy vs. clean training sets\n",
      "We begin by training our\n",
      "best model (baseline+local+global) on all training sets of\n",
      "Table 2, except NC-noisy because some images are cur-\n",
      "rently unavailable. As shown in Table 3, even though\n",
      "TRAIN SET\n",
      "#IMAGES\n",
      "#CLASSES\n",
      "NC-noisy\n",
      "213,678\n",
      "672\n",
      "NC-clean\n",
      "27,965\n",
      "581\n",
      "SfM-120k\n",
      "117,369\n",
      "713\n",
      "GLDv1-noisy\n",
      "1,225,029\n",
      "14, 951\n",
      "GLDv2-noisy\n",
      "4,132,914\n",
      "203,094\n",
      "GLDv2-clean\n",
      "1,580,470\n",
      "81,313\n",
      "Table 2: Statistics of different training sets.\n",
      "METHOD\n",
      "TRAIN SET\n",
      "DIM OXF5K PAR6K RMEDIUM\n",
      "RHARD\n",
      "ROxf RPar ROxf RPar\n",
      "GeM-Siamese [37, 35]\n",
      "SfM-120k\n",
      "2048\n",
      "87.8\n",
      "92.7\n",
      "64.7\n",
      "77.2\n",
      "38.5\n",
      "56.3\n",
      "SOLAR [28]\n",
      "GLDv1-noisy 2048\n",
      "‚Äì\n",
      "‚Äì\n",
      "69.9\n",
      "81.6\n",
      "47.9\n",
      "64.5\n",
      "GLDv2 [53]\n",
      "GLDv2-clean 2048\n",
      "‚Äì\n",
      "‚Äì\n",
      "74.2\n",
      "84.9\n",
      "51.6\n",
      "70.3\n",
      "GLAM (Ours)\n",
      "NC-clean\n",
      "512\n",
      "77.8\n",
      "85.8\n",
      "51.6\n",
      "68.1\n",
      "20.9\n",
      "44.7\n",
      "GLDv1-noisy 512\n",
      "92.8\n",
      "95.0\n",
      "73.7\n",
      "83.5\n",
      "49.8\n",
      "69.4\n",
      "GLDv2-noisy 512\n",
      "93.3\n",
      "95.3\n",
      "75.7\n",
      "86.0\n",
      "53.1\n",
      "73.8\n",
      "GLDv2-clean 512\n",
      "94.2\n",
      "95.6\n",
      "78.6\n",
      "88.5\n",
      "60.2\n",
      "76.8\n",
      "Table 3: mAP comparison of our best model (base-\n",
      "line+local+global) trained on different training sets against\n",
      "[53, 28]. All models use ResNet101-GeM. Red: best results.\n",
      "Blue: GLAM higher than SOLAR [28] on GLDv1-noisy.\n",
      "GLDv2-noisy has 2.6 times more images than GLDv2-\n",
      "clean, the latter is superior by a large margin. This shows\n",
      "that, in training, a cleaner dataset can be more important\n",
      "than a larger one. By contrast, NC-clean has the worst\n",
      "performance despite being clean, aparently because it is\n",
      "6\n",
      "METHOD\n",
      "TRAIN SET\n",
      "DIM\n",
      "BASE\n",
      "MEDIUM\n",
      "HARD\n",
      "Ox5k Par6k\n",
      "ROxf\n",
      "+R1M\n",
      "RPar\n",
      "+R1M\n",
      "ROxf\n",
      "+R1M\n",
      "RPar\n",
      "+R1M\n",
      "mAP\n",
      "mAP mAP mP mAP mP mAP mP mAP mP\n",
      "mAP mP mAP mP mAP mP mAP mP\n",
      "SPoC-V16 [2, 35]\n",
      "[O]\n",
      "512\n",
      "53.1‚àó\n",
      "‚Äì\n",
      "38.0 54.6 17.1 33.3 59.8 93.0 30.3 83.0 11.4 20.9\n",
      "0.9\n",
      "2.9\n",
      "32.4 69.7\n",
      "7.6\n",
      "30.6\n",
      "SPoC-R101 [35]\n",
      "[O]\n",
      "2048\n",
      "‚Äì\n",
      "‚Äì\n",
      "39.8 61.0 21.5 40.4 69.2 96.7 41.6 92.0 12.4 23.8\n",
      "2.8\n",
      "5.6\n",
      "44.7 78.0 15.3 54.4\n",
      "CroW-V16 [24, 35]\n",
      "[O]\n",
      "512\n",
      "70.8\n",
      "79.7\n",
      "41.4 58.8 22.5 40.5 62.9 94.4 34.1 87.1 13.9 25.7\n",
      "3.0\n",
      "6.6\n",
      "36.9 77.9 10.3 45.1\n",
      "CroW-R101 [35]\n",
      "[O]\n",
      "2048\n",
      "‚Äì\n",
      "‚Äì\n",
      "42.4 61.9 21.2 39.4 70.4 97.1 42.7 92.9 13.3 27.7\n",
      "3.3\n",
      "9.3\n",
      "47.2 83.6 16.3 61.6\n",
      "MAC-V16-Siamese [36, 35]\n",
      "[O]\n",
      "512\n",
      "80.0\n",
      "82.9\n",
      "37.8 57.8 21.8 39.7 59.2 93.3 33.6 87.1 14.6 27.0\n",
      "7.4\n",
      "11.9 35.9 78.4 13.2 54.7\n",
      "MAC-R101-Siamese [35]\n",
      "[O]\n",
      "2048\n",
      "‚Äì\n",
      "‚Äì\n",
      "41.7 65.0 24.2 43.7 66.2 96.4 40.8 93.0 18.0 32.9\n",
      "5.7\n",
      "14.4 44.1 86.3 18.2 67.7\n",
      "RMAC-V16-Siamese [36, 35]\n",
      "[O]\n",
      "512\n",
      "80.1\n",
      "85.0\n",
      "42.5 62.8 21.7 40.3 66.2 95.4 39.9 88.9 12.0 26.1\n",
      "1.7\n",
      "5.8\n",
      "40.9 77.1 14.8 54.0\n",
      "RMAC-R101-Siamese [35]\n",
      "[O]\n",
      "2048\n",
      "‚Äì\n",
      "‚Äì\n",
      "49.8 68.9 29.2 48.9 74.0 97.7 49.3 93.7 18.5 32.2\n",
      "4.5\n",
      "13.0 52.1 87.1 21.3 67.4\n",
      "RMAC-R101-Triplet [16, 35]\n",
      "NC-clean\n",
      "2048\n",
      "86.1\n",
      "94.5\n",
      "60.9 78.1 39.3 62.1 78.9 96.9 54.8 93.9 32.4 50.0 12.5 24.9 59.4 86.1 28.0 70.0\n",
      "GeM-R101-Siamese [37, 35]\n",
      "SfM-120k\n",
      "2048\n",
      "87.8\n",
      "92.7\n",
      "64.7 84.7 45.2 71.7 77.2 98.1 52.3 95.3 38.5 53.0 19.9 34.9 56.3 89.1 24.7 73.3\n",
      "AGeM-R101-Siamese [17]\n",
      "SfM-120k\n",
      "2048\n",
      "‚Äì\n",
      "‚Äì\n",
      "67.0\n",
      "‚Äì\n",
      "‚Äì\n",
      "‚Äì\n",
      "78.1\n",
      "‚Äì\n",
      "‚Äì\n",
      "‚Äì\n",
      "40.7\n",
      "‚Äì\n",
      "‚Äì\n",
      "‚Äì\n",
      "57.3\n",
      "‚Äì\n",
      "‚Äì\n",
      "‚Äì\n",
      "SOLAR-GeM-R101-Triplet/SOS [28] GLDv1-noisy 2048\n",
      "‚Äì\n",
      "‚Äì\n",
      "69.9 86.7 53.5 76.7 81.6 97.1 59.2 94.9 47.9 63.0 29.9 48.9 64.5 93.0 33.4 81.6\n",
      "DELG-GeM-R101-ArcFace [5]\n",
      "GLDv1-noisy 2048\n",
      "‚Äì\n",
      "‚Äì\n",
      "73.2\n",
      "‚Äì\n",
      "54.8\n",
      "‚Äì\n",
      "82.4\n",
      "‚Äì\n",
      "61.8\n",
      "‚Äì\n",
      "51.2\n",
      "‚Äì\n",
      "30.3\n",
      "‚Äì\n",
      "64.7\n",
      "‚Äì\n",
      "35.5\n",
      "‚Äì\n",
      "GeM-R101-ArcFace [53]\n",
      "GLDv2-clean 2048\n",
      "‚Äì\n",
      "‚Äì\n",
      "74.2\n",
      "‚Äì\n",
      "‚Äì\n",
      "‚Äì\n",
      "84.9\n",
      "‚Äì\n",
      "‚Äì\n",
      "‚Äì\n",
      "51.6\n",
      "‚Äì\n",
      "‚Äì\n",
      "‚Äì\n",
      "70.3\n",
      "‚Äì\n",
      "‚Äì\n",
      "‚Äì\n",
      "GLAM-GeM-R101-ArcFace baseline\n",
      "GLDv2-clean\n",
      "512\n",
      "91.9\n",
      "94.5\n",
      "72.8 86.7 58.1 78.2 84.2 95.9 63.9 93.3 49.9 62.1 31.6 49.7 69.7 88.4 37.7 73.7\n",
      "+local\n",
      "GLDv2-clean\n",
      "512\n",
      "91.2\n",
      "95.4\n",
      "73.7 86.2 60.5 77.4 86.5 95.6 68.0 93.9 52.6 65.3 36.1 55.6 73.7 89.3 44.7 79.1\n",
      "+global\n",
      "GLDv2-clean\n",
      "512\n",
      "92.3\n",
      "95.3\n",
      "77.2 87.0 63.8 79.3 86.7 95.4 67.8 93.7 57.4 69.6 38.7 57.9 75.0 89.4 45.0 77.0\n",
      "+global+local\n",
      "GLDv2-clean\n",
      "512\n",
      "94.2\n",
      "95.6\n",
      "78.6 88.2 68.0 82.4 88.5 97.0 73.5 94.9 60.2 72.9 43.5 62.1 76.8 93.4 53.1 84.0\n",
      "Table 4: mAP comparison of our GLAM against SOTA methods based on global descriptors without re-ranking. V16:\n",
      "VGG16; R101: ResNet101. [O]: Off-the-shelf (pre-trained on ImageNet). ‚àó: dimension d = 256 [2]. mP: mP@10. Red:\n",
      "best results. Black bold: best previous methods. Blue: GLAM higher than previous methods. Weyand et al. [53] is the only\n",
      "model other than ours trained on GLDv2-clean, while [28] is trained on GLDv1-noisy and compared in Table 3.\n",
      "too small. To achieve best possible performance, we use\n",
      "GLDv2-clean as a training set in the remaining experiments.\n",
      "Comparisons on same training set\n",
      "It is common to com-\n",
      "pare methods regardless of training sets as more become\n",
      "available, e.g., [35, 28]. Since GLDv2-clean is relatively\n",
      "new, Weyand et al. [53], which introduced the dataset, is the\n",
      "only study that has trained the same backbone with the same\n",
      "settings (ResNet101-GeM with ArcFace) on GLDv2-clean.\n",
      "Our baseline is lower than [53], because our dimensinality is\n",
      "512, while other models based on ResNet101 use 2048. Yet,\n",
      "Table 3 shows that our best model trained on GLDv2-clean\n",
      "outperforms [53] by a large margin. But the most impor-\n",
      "tant comparison is with SOLAR [28], also based on self-\n",
      "attention, which has trained ResNet101-GeM on GLDv1-\n",
      "noisy. On this training set, our best model clearly outper-\n",
      "forms [28] despite lower dimensionality.\n",
      "Comparison with state of the art\n",
      "Table 4 shows the\n",
      "performance of four variants of our model, i.e. baseline\n",
      "with or without local/global attention, and compares them\n",
      "against state-of-the-art (SOTA) methods based on global de-\n",
      "scriptors without re-ranking on the complete set of bench-\n",
      "marks, including distractors. Both local and global atten-\n",
      "tion bring signiÔ¨Åcant gain over the baseline. The effect\n",
      "of global is stronger, while the gain of the two is addi-\n",
      "tive in the combination. The best results are achieved by\n",
      "the global-local attention network (baseline+global+local).\n",
      "With this model, we outperform previous best methods\n",
      "on most benchmarks except mP@10 on RParis (medium)\n",
      "and RParis+R1M (medium), where we are outperformed\n",
      "by [37, 35]. These results demonstrate that our approach is\n",
      "effective for landmark image retrieval. Figure 7 shows some\n",
      "METHOD\n",
      "OXF5K PAR6K RMEDIUM\n",
      "RHARD\n",
      "ROxf RPar ROxf RPar\n",
      "GLAM baseline\n",
      "91.9\n",
      "94.5\n",
      "72.8\n",
      "84.2\n",
      "49.9\n",
      "69.7\n",
      "+local-channel\n",
      "91.3\n",
      "95.3\n",
      "72.2\n",
      "85.8\n",
      "48.3\n",
      "73.1\n",
      "+local-spatial\n",
      "91.0\n",
      "95.1\n",
      "72.1\n",
      "85.3\n",
      "48.3\n",
      "71.9\n",
      "+local\n",
      "91.2\n",
      "95.4\n",
      "73.7\n",
      "86.5\n",
      "52.6\n",
      "75.0\n",
      "+global-channel\n",
      "92.5\n",
      "94.4\n",
      "73.3\n",
      "84.4\n",
      "49.8\n",
      "70.1\n",
      "+global-spatial\n",
      "92.4\n",
      "95.1\n",
      "73.2\n",
      "86.3\n",
      "50.0\n",
      "72.7\n",
      "+global\n",
      "92.3\n",
      "95.3\n",
      "77.2\n",
      "86.7\n",
      "57.4\n",
      "75.0\n",
      "+global+local\n",
      "94.2\n",
      "95.6\n",
      "78.6\n",
      "88.5\n",
      "60.2\n",
      "76.8\n",
      "Table 5: mAP comparison of spatial and channel variants\n",
      "of our local (+local, subsection 3.1) and global (+global,\n",
      "subsection 3.1) attention modules to the baseline.\n",
      "METHOD\n",
      "OXF5K\n",
      "PAR6K\n",
      "RMEDIUM\n",
      "RHARD\n",
      "ROxf RPar ROxf RPar\n",
      "CBAM style\n",
      "93.8\n",
      "95.7\n",
      "75.6\n",
      "88.4\n",
      "53.3\n",
      "76.8\n",
      "GLAM (Ours)\n",
      "94.2\n",
      "95.6\n",
      "78.6\n",
      "88.5\n",
      "60.2\n",
      "76.8\n",
      "Table 6: mAP comparison between CBAM style and our\n",
      "local spatial attention.\n",
      "examples of our ranking results.\n",
      "4.4. Ablation study\n",
      "Our ablation study uses the Google Landmark v2 clean\n",
      "dataset (GLDv2-clean) [53] for training, which is shown to\n",
      "be the most effective in Table 3.\n",
      "7\n",
      "METHOD\n",
      "OXF5K\n",
      "PAR6K\n",
      "RMEDIUM\n",
      "RHARD\n",
      "ROxf\n",
      "RPar\n",
      "ROxf\n",
      "RPar\n",
      "Concatenate\n",
      "89.5\n",
      "95.1\n",
      "73.6\n",
      "86.5\n",
      "54.0\n",
      "73.7\n",
      "Sum (Ours)\n",
      "94.2\n",
      "95.6\n",
      "78.6\n",
      "88.5\n",
      "60.2\n",
      "76.8\n",
      "Table 7: mAP comparison between weighted concatenation\n",
      "and weighted average for feature fusion.\n",
      "METHOD\n",
      "OXF5K PAR6K\n",
      "RMEDIUM\n",
      "RHARD\n",
      "ROxf RPar ROxf RPar\n",
      "Fixed-size\n",
      "76.1\n",
      "82.6\n",
      "55.7\n",
      "68.4\n",
      "29.2\n",
      "47.5\n",
      "Group-size (Ours)\n",
      "94.2\n",
      "95.6\n",
      "78.6\n",
      "88.5\n",
      "60.2\n",
      "76.8\n",
      "Table 8: mAP comparison between Ô¨Åxed-size (224 √ó 224)\n",
      "and group-size sampling methods.\n",
      "QUERY DATABASE OXF5K PAR6K RMEDIUM\n",
      "RHARD\n",
      "ROxf RPar ROxf RPar\n",
      "Single\n",
      "Single\n",
      "93.3\n",
      "95.2\n",
      "76.9\n",
      "87.1\n",
      "58.6\n",
      "74.7\n",
      "Multi\n",
      "Single\n",
      "93.9\n",
      "95.4\n",
      "78.0\n",
      "87.7\n",
      "59.0\n",
      "75.5\n",
      "Single\n",
      "Multi\n",
      "93.6\n",
      "95.6\n",
      "77.0\n",
      "87.8\n",
      "57.1\n",
      "76.0\n",
      "Multi\n",
      "Multi\n",
      "94.2\n",
      "95.6\n",
      "78.6\n",
      "88.5\n",
      "60.2\n",
      "76.8\n",
      "Table 9: mAP comparison of using multiresolution repre-\n",
      "sentation (Multi) or not (Single) on query or database.\n",
      "Effect of attention modules\n",
      "We ablate the effect of our\n",
      "local and global attention networks as well as their com-\n",
      "bination. Table 5 shows the results, which are more Ô¨Åne-\n",
      "grained than those of Table 4. In particular, it shows the ef-\n",
      "fect of the channel and spatial variants of both local and\n",
      "global attention. We observe that, when used alone, the\n",
      "channel and spatial variants of local attention are harmful\n",
      "in most cases. Even the combination, baseline+local, is not\n",
      "always effective. By contrast, when used alone, the channel\n",
      "and spatial variants of global attention are mostly beneÔ¨Åcial,\n",
      "especially the latter. Their combination, baseline+global, is\n",
      "impressive, bringing gain of up to 7.5%. Importantly, the\n",
      "combination baseline+global+local improves further by up\n",
      "to another 2.8%. This result shows the necessity of local\n",
      "attention in the Ô¨Ånal model.\n",
      "CBAM vs. our local spatial attention\n",
      "We experiment\n",
      "with the local spatial attention of CBAM [54]. CBAM ap-\n",
      "plies average and max-pooling to input features and con-\n",
      "catenates the two for spatial attention. We apply this vari-\n",
      "ant to our local spatial attention module for comparison.\n",
      "For the CBAM style module, we keep the overall design\n",
      "of our module as shown in Figure 3, but apply average and\n",
      "max-pooling to each of the four convolutional layer outputs\n",
      "before concatenation. Table 6 shows that the CBAM style\n",
      "module is considerably worse than ours on all benchmarks\n",
      "except Paris6k, where it is only slightly better.\n",
      "Concatenation vs. sum for feature fusion\n",
      "We use a\n",
      "softmax-based weighted average of local and global atten-\n",
      "tion feature maps with the original feature map (7). Here,\n",
      "we compare this weighted average with weighted concate-\n",
      "nation, where concatenation replaces the sum operation\n",
      "in (7). As shown in Table 7, the weighted average outper-\n",
      "forms the weighted concatenation.\n",
      "Fixed-size vs. group-size sampling\n",
      "Numerous studies\n",
      "have proposed methods for constructing batches according\n",
      "to image size for efÔ¨Åcient training. For instance, Gordo et\n",
      "al. [16], DELF [29], and Yokoo et al. [56] employed dif-\n",
      "ferent image sizes per batch for training instead of a single\n",
      "Ô¨Åxed size. We adopt the method of Yokoo et al., which con-\n",
      "structs a batch with images of similar aspect ratio, so that\n",
      "the images can be resized to a size with an aspect ratio that\n",
      "is similar to their own. We call this method group-size sam-\n",
      "pling. Table 8 compares Ô¨Åxed-size (224 √ó 224) with group-\n",
      "size sampling. We observe that maintaining aspect ratios by\n",
      "using dynamic input sizes is much more effective.\n",
      "Multi-resolution\n",
      "We use the multi-resolution representa-\n",
      "tion [16] for the Ô¨Ånal feature of an image at inference time.\n",
      "This method: (1) resizes an image into multiple scales; (2)\n",
      "extracts features from the resized images; and (3) averages\n",
      "the features to obtain the Ô¨Ånal feature of the image. The\n",
      "method is applied to both query and database images to en-\n",
      "hance ranking results, especially for small target objects.\n",
      "Table 9 compares the four cases of applying this method or\n",
      "not to query or database images.\n",
      "5. Conclusion\n",
      "We have introduced a novel approach that extracts global\n",
      "and local contextual information using attention mecha-\n",
      "nisms for instance-level image retrieval. It is manifested as\n",
      "a network architecture consisting of global and local atten-\n",
      "tion components, each operating on both spatial and chan-\n",
      "nel dimensions. This constitutes a comprehensive study and\n",
      "empirical evaluation of all four forms of attention that have\n",
      "previously been studied only in isolation. Our Ô¨Åndings indi-\n",
      "cate that the gain (or loss) brought by one form of attention\n",
      "alone strongly depends on the presence of the others, with\n",
      "the maximum gain appearing when all forms are present.\n",
      "The output is a modiÔ¨Åed feature tensor that can be used in\n",
      "any way, for instance with local feature detection instead of\n",
      "spatial pooling for image retrieval.\n",
      "With the advent of vision transformers [12, 58] and their\n",
      "recent application to image retrieval [13], attention is ex-\n",
      "pected to play a more and more signiÔ¨Åcant role in vi-\n",
      "sion. According to our classiÔ¨Åcation, transformers perform\n",
      "global spatial attention alone. It is of great interest to in-\n",
      "vestigate the role of the other forms of attention, where our\n",
      "8\n",
      "approach may yield a basic building block of such archi-\n",
      "tectures. One may even envision an extension to language\n",
      "models, where transformers originate from [50].\n",
      "References\n",
      "[1] Relja Arandjelovi¬¥c, Petr Gronat, Akihiko Torii, Tomas Pa-\n",
      "jdla, and Josef Sivic.\n",
      "NetVLAD: CNN architecture for\n",
      "weakly supervised place recognition. In CVPR, 2016. 2\n",
      "[2] Artem Babenko and Victor Lempitsky. Aggregating Local\n",
      "Deep Features for Image Retrieval. In ICCV, 2015. 1, 2, 7\n",
      "[3] Artem Babenko, Anton Slesarev, Alexandr Chigorin, and\n",
      "Victor Lempitsky.\n",
      "Neural Codes for Image Retrieval.\n",
      "In\n",
      "ECCV, 2014. 1, 2, 5\n",
      "[4] Irwan Bello, Barret Zoph, Ashish Vaswani, Jonathon Shlens,\n",
      "and Quoc V. Le.\n",
      "Attention augmented convolutional net-\n",
      "works. In ICCV, 2019. 2, 3\n",
      "[5] Bingyi Cao, Andr¬¥e Araujo, and Jack Sim. Unifying deep\n",
      "local and global features for image search. In ECCV, 2020.\n",
      "2, 3, 7\n",
      "[6] Yue Cao, Jiarui Xu, Stephen Lin, Fangyun Wei, and Han Hu.\n",
      "GCNet: Non-Local Networks Meet Squeeze-Excitation Net-\n",
      "works and Beyond. In ICCV, 2019. 2\n",
      "[7] Liang-Chieh Chen, George Papandreou, Florian Schroff, and\n",
      "Hartwig Adam. Rethinking atrous convolution for seman-\n",
      "tic image segmentation. arXiv preprint arXiv:1706.05587,\n",
      "2017. 4\n",
      "[8] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\n",
      "offrey Hinton. A simple framework for contrastive learning\n",
      "of visual representations. In ICML, 2020. 1\n",
      "[9] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng\n",
      "Yan, and Jiashi Feng. AÀÜ2-nets: Double attention networks.\n",
      "In NeurIPS, 2018. 1, 2, 3, 4\n",
      "[10] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos\n",
      "Zafeiriou. ArcFace: Additive Angular Margin Loss for Deep\n",
      "Face Recognition. In CVPR, 2019. 6\n",
      "[11] Michael Donoser and Horst Bischof. Diffusion Processes for\n",
      "Retrieval Revisited. In CVPR, 2013. 1, 2, 6\n",
      "[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\n",
      "Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\n",
      "Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\n",
      "vain Gelly, et al. An image is worth 16x16 words: Trans-\n",
      "formers for image recognition at scale.\n",
      "arXiv preprint\n",
      "arXiv:2010.11929, 2020. 8\n",
      "[13] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and\n",
      "Herv¬¥e J¬¥egou.\n",
      "Training vision transformers for image re-\n",
      "trieval. Technical report, 2021. 8\n",
      "[14] Zilin Gao, Jiangtao Xie, Qilong Wang, and Peihua Li. Global\n",
      "second-order pooling convolutional networks.\n",
      "In CVPR,\n",
      "2019. 2, 3, 4\n",
      "[15] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\n",
      "lus. Deep image retrieval: Learning global representations\n",
      "for image search. In ECCV, 2016. 2\n",
      "[16] Albert Gordo, Jon Almazan, Jerome Revaud, and Diane Lar-\n",
      "lus. End-to-end learning of deep visual representations for\n",
      "image retrieval. IJCV, 2017. 1, 2, 5, 6, 7, 8\n",
      "[17] Yinzheng Gu, Chuanpeng Li, and Jinbin Xie.\n",
      "Attention-\n",
      "aware generalized mean pooling for image retrieval. arXiv\n",
      "preprint arXiv:1811.00202, 2018. 2, 3, 7\n",
      "[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\n",
      "Deep residual learning for image recognition.\n",
      "In CVPR,\n",
      "2016. 6\n",
      "[19] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea\n",
      "Vedaldi. Gather-excite: Exploiting feature context in con-\n",
      "volutional neural networks. In NeurIPS, 2018. 2\n",
      "[20] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Enhua Wu.\n",
      "Squeeze-and-Excitation Networks. In CVPR, 2018. 1, 2\n",
      "[21] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, Teddy Furon,\n",
      "and Ondrej Chum. EfÔ¨Åcient diffusion on region manifolds:\n",
      "Recovering small objects with compact cnn representations.\n",
      "In CVPR, 2017. 2, 6\n",
      "[22] H. Jegou, F. Perronnin, M. Douze, J. Sanchez, P. Perez, and\n",
      "C. Schmid. Aggregating local image descriptors into com-\n",
      "pact codes. PAMI, (99):1‚Äì1, 2011. 2\n",
      "[23] Albert Jimenez, Jose M. Alvarez, and Xavier Gir¬¥o-i-Nieto.\n",
      "Class weighted convolutional features for visual instance\n",
      "search. In BMVC, 2017. 2, 3\n",
      "[24] Yannis Kalantidis, Clayton Mellina, and Simon Osindero.\n",
      "Crossdimensional weighting for aggregated deep convolu-\n",
      "tional features. In ECCV, 2016. 1, 2, 3, 7\n",
      "[25] Hyo Jin Kim, Enrique Dunn, and Jan-Michael Frahm.\n",
      "Learned Contextual Feature Reweighting for Image Geo-\n",
      "Localization. In CVPR, 2017. 1, 2, 3\n",
      "[26] Sungyeon Kim, Dongwon Kim, Minsu Cho, and Suha Kwak.\n",
      "Proxy anchor loss for deep metric learning. In CVPR, 2020.\n",
      "1\n",
      "[27] David G. Lowe.\n",
      "Distinctive image features from scale-\n",
      "invariant keypoints. In IJCV, 2004. 1, 2\n",
      "[28] Tony Ng, Vassileios Balntas, Yurun Tian, and Krystian\n",
      "Mikolajczyk. SOLAR: Second-Order Loss and Attention for\n",
      "Image Retrieval. In ECCV, 2020. 1, 2, 3, 5, 6, 7\n",
      "[29] Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand,\n",
      "and Bohyung Han. Large Scale Image Retrieval with Atten-\n",
      "tive Deep Local Features. In ICCV, 2017. 1, 2, 3, 5, 6, 8\n",
      "[30] Hyun Oh Song, Yu Xiang, Stefanie Jegelka, and Silvio\n",
      "Savarese. Deep metric learning via lifted structured feature\n",
      "embedding. In CVPR, 2016. 1\n",
      "[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\n",
      "James Bradbury, Gregory Chanan, Trevor Killeen, Zeming\n",
      "Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison,\n",
      "Andreas K¬®opf, Edward Yang, Zach DeVito, Martin Raison,\n",
      "Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu\n",
      "Fang, Junjie Bai, and Soumith Chintala. PyTorch: An im-\n",
      "perative style, high-performance deep learning. In NeurIPS,\n",
      "2019. 5\n",
      "[32] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\n",
      "Andrew Zisserman. Object retrieval with large vocabularies\n",
      "and fast spatial matching. In CVPR, 2007. 2, 5\n",
      "[33] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and\n",
      "Andrew Zisserman. Lost in quantization:Improving particu-\n",
      "lar object retrieval in large scale image databases. In CVPR,\n",
      "2008. 5\n",
      "9\n",
      "[34] Tobias Pl¬®otz and Stefan Roth. Neural nearest neighbors net-\n",
      "works. In NeurIPS, 2018. 2, 3\n",
      "[35] Filip Radenovi¬¥c, Ahmet Iscen, Giorgos Tolias, Yannis\n",
      "Avrithis, and OndÀárej Chum. Revisiting Oxford and Paris:\n",
      "Large-Scale Image Retrieval Benchmarking. In CVPR, 2018.\n",
      "5, 6, 7\n",
      "[36] Filip Radenovi¬¥c, Giorgos Tolias, and OndÀárej Chum. CNN\n",
      "image retrieval learns from BoW: Unsupervised Ô¨Åne-tuning\n",
      "with hard examples. In ECCV, 2016. 2, 7\n",
      "[37] Filip Radenovi¬¥c, Giorgos Tolias, and OndÀárej Chum. Fine-\n",
      "Tuning CNN Image Retrieval with No Human Annotation.\n",
      "In TPAMI, 2019. 1, 2, 5, 6, 7\n",
      "[38] Ali Sharif Razavian, Josephine Sullivan, Stefan Carlsson,\n",
      "and Atsuto Maki. Visual Instance Retrieval with Deep Con-\n",
      "volutional Networks. In CoRR, 2015. 2\n",
      "[39] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\n",
      "jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\n",
      "Aditya Khosla, Michael Bernstein, Alexander C. Berg, and\n",
      "Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal-\n",
      "lenge. In International booktitle of Computer Vision, 2015.\n",
      "5\n",
      "[40] Oriane Sim¬¥eoni, Yannis Avrithis, and Ondrej Chum. Local\n",
      "features and visual words emerge in activations. In CVPR,\n",
      "2019. 2, 6\n",
      "[41] O. Sim¬¥eoni, A. Iscen, G. Tolias, Y. Avrithis, and O. Chum.\n",
      "Graph-based particular object discovery. Machine Vision and\n",
      "Applications, 30(2):243‚Äì254, 3 2019. 3\n",
      "[42] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototyp-\n",
      "ical networks for few-shot learning. In NeurIPS, 2017. 1\n",
      "[43] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\n",
      "Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\n",
      "Vanhoucke, and Andrew Rabinovich.\n",
      "Going deeper with\n",
      "convolutions. In CVPR, 2015. 3\n",
      "[44] Mingxing Tan, Ruoming Pang, and Quoc V. Le. EfÔ¨ÅcientDet:\n",
      "Scalable and EfÔ¨Åcient Object Detection. In CVPR, 2020. 5\n",
      "[45] Marvin Teichmann, Andre Araujo, Menglong Zhu, and Jack\n",
      "Sim.\n",
      "Detect-to-retrieve: EfÔ¨Åcient regional aggregation for\n",
      "image search. In CVPR, 2019. 2\n",
      "[46] Giorgios Tolias, Yannis Avrithis, and Herv¬¥e J¬¥egou. To aggre-\n",
      "gate or not to aggregate: Selective match kernels for image\n",
      "search. In ICCV, 2013. 2\n",
      "[47] Giorgos Tolias, Tomas Jenicek, and OndÀárej Chum. Learn-\n",
      "ing and aggregating deep local descriptors for instance-level\n",
      "recognition. In ECCV, 2020. 2, 3\n",
      "[48] Giorgos Tolias, Ronan Sicre, and Herv¬¥e J¬¥egou. Particular ob-\n",
      "ject retrieval with integral max-pooling of CNN activations.\n",
      "In ICLR, 2016. 2\n",
      "[49] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\n",
      "reit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\n",
      "Polosukhin. Attention is all you need. In NeurIPS, 2017. 4\n",
      "[50] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\n",
      "reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\n",
      "Polosukhin. Attention is all you need. In NeurIPS, 2017. 9\n",
      "[51] Qilong Wang, Banggu Wu, Pengfei Zhu, Peihua Li, Wang-\n",
      "meng Zuo, and Qinghua Hu.\n",
      "ECA-Net: EfÔ¨Åcient Chan-\n",
      "nel Attention for Deep Convolutional Neural Networks. In\n",
      "CVPR, 2020. 2, 3, 4\n",
      "[52] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\n",
      "ing He. Non-local Neural Networks. In CVPR, 2018. 1, 2,\n",
      "3, 4\n",
      "[53] Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim.\n",
      "Google Landmarks Dataset v2 - A Large-Scale Benchmark\n",
      "for Instance-Level Recognition and Retrieval.\n",
      "In CVPR,\n",
      "2020. 1, 2, 5, 6, 7\n",
      "[54] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So\n",
      "Kweon. CBAM: Convolutional Block Attention Module. In\n",
      "ECCV, 2018. 1, 2, 4, 8\n",
      "[55] Fan Yang, Ryota Hinami, Yusuke Matsui, Steven Ly, and\n",
      "Shin‚Äôichi Satoh. EfÔ¨Åcient image retrieval via decoupling dif-\n",
      "fusion into online and ofÔ¨Çine processing. In AAAI, 2019. 2,\n",
      "6\n",
      "[56] Shuhei Yokoo, Kohei Ozaki, Edgar Simo-Serra, and Satoshi\n",
      "Iizuka. Two-stage Discriminative Re-ranking for Large-scale\n",
      "Landmark Retrieval. In arXiv:2003.11211, 2020. 6, 8\n",
      "[57] Fisher Yu, Vladlen Koltun, and Thomas Funkhouser. Dilated\n",
      "residual networks. In CVPR, 2017. 4\n",
      "[58] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\n",
      "Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\n",
      "to-token vit: Training vision transformers from scratch on\n",
      "imagenet. arXiv preprint arXiv:2101.11986, 2021. 8\n",
      "[59] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\n",
      "self-attention for image recognition. In CVPR, 2020. 2, 3\n",
      "10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the search_arxiv_papers function\n",
    "search_arxiv_papers(\"Attention is all you need\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cc2a88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
